{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "root_path = '/mnt/c/Users/121js/OneDrive/Desktop/TorchImages/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Critic and Generator classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_classes, img_size, img_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.crit = nn.Sequential(\n",
    "            self._make_conv2d_block(img_channels+1, features, use_bn=False),\n",
    "\n",
    "            self._make_conv2d_block(features, features*2),\n",
    "            self._make_conv2d_block(features*2, features*4),\n",
    "            self._make_conv2d_block(features*4, features*8),\n",
    "\n",
    "            self._make_conv2d_block(features*8, 1, 4, 2, 0, use_act=False, use_bn=False),\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, img_size*img_size)\n",
    "\n",
    "    def _make_conv2d_block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, use_bn=True, use_act=True, leak=0.2):\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "        ]\n",
    "        if use_bn:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        if use_act:\n",
    "            layers.append(nn.LeakyReLU(leak))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, embedding], axis=1)\n",
    "        return self.crit(x)\n",
    "    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_classes, embed_size, z_dim=100, features=64, img_channels=3) -> None:\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._make_convT2d_block(z_dim+embed_size, features*8, 4, 1, 0),\n",
    "\n",
    "            self._make_convT2d_block(features*8, features*4),\n",
    "            self._make_convT2d_block(features*4, features*2),\n",
    "            self._make_convT2d_block(features*2, features),\n",
    "\n",
    "            self._make_convT2d_block(features, img_channels, use_bn=False, use_act=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, embed_size)\n",
    "\n",
    "    def _make_convT2d_block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, use_bn=True, use_act=True):\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            )\n",
    "        ]\n",
    "        if use_bn:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        if use_act:\n",
    "            layers.append(nn.ReLU())\n",
    "        return nn.Sequential(*layers)        \n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([x, embedding], dim=1)\n",
    "        return self.gen(x)\n",
    "    \n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m , (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(\n",
    "                tensor=m.weight.data,\n",
    "                mean=0.0,\n",
    "                std=0.02,\n",
    "            )\n",
    "\n",
    "def find_gradient_penalty(critic, labels, real, fake, device='gpu'):\n",
    "    b, c, h, w = real.shape\n",
    "    alpha = torch.rand((b, 1, 1, 1)).repeat(1, c, h, w).to(device)\n",
    "    interpolated_batch = real*alpha + fake*(1-alpha)\n",
    "    critic_scores = critic(interpolated_batch, labels)\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        outputs=critic_scores,\n",
    "        inputs=interpolated_batch,\n",
    "        grad_outputs=torch.ones_like(critic_scores),\n",
    "        retain_graph=True,\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm-1)**2)\n",
    "    return gradient_penalty\n",
    "\n",
    "def save_checkpoint(state, filename='mnist_wgangp.pth.tar'):\n",
    "    print(\"=> Saving Checkpoint <=\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, gen, crit):\n",
    "    print(\"=> Loading Checkpoint <=\")\n",
    "    gen.load_state_dict(checkpoint['gen'])\n",
    "    crit.load_state_dict(checkpoint['crit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE, NUM_CHANNELS, H, W = 8, 3, 64, 64\n",
    "# Z_DIM = 100\n",
    "# FEATURES = 64\n",
    "\n",
    "# img_test = torch.randn((BATCH_SIZE, NUM_CHANNELS, H, W))\n",
    "# crit = Critic(10, 64, NUM_CHANNELS, FEATURES)\n",
    "# assert crit(img_test).shape == (BATCH_SIZE, 1, 1, 1), 'Crit Test Failed'\n",
    "\n",
    "# noise_test = torch.randn((BATCH_SIZE, Z_DIM, 1, 1))\n",
    "# gen = Generator(10, 100, Z_DIM, FEATURES, NUM_CHANNELS)\n",
    "# assert gen(noise_test).shape == (BATCH_SIZE, NUM_CHANNELS, H, W), 'Gen Test Failed'\n",
    "\n",
    "# print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 128\n",
    "NUM_CHANNELS = 1\n",
    "IMG_SIZE = 64\n",
    "NUM_CLASSES = 10\n",
    "GEN_EMBEDDING = 100\n",
    "Z_DIM = 100\n",
    "FEATURES_CRIT = 64\n",
    "FEATURES_GEN = 128\n",
    "LR = 1e-4\n",
    "NUM_EPOCHS = 50\n",
    "CRIT_ITER = 5\n",
    "GRID_SHOW = 10\n",
    "LAMBDA_GP = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = Critic(NUM_CLASSES, IMG_SIZE, NUM_CHANNELS, FEATURES_CRIT).to(device)\n",
    "gen = Generator(NUM_CLASSES, GEN_EMBEDDING, Z_DIM, FEATURES_GEN, NUM_CHANNELS).to(device)\n",
    "initialize_weights(crit), initialize_weights(gen)\n",
    "fixed_labels = torch.arange(10).to(device)\n",
    "\n",
    "transformations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(NUM_CHANNELS)], [0.5 for _ in range(NUM_CHANNELS)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# images = datasets.ImageFolder(root=root_path + 'celebs', transform=transformations)\n",
    "images = datasets.MNIST(root='mnist', train=True, transform=transformations, download=True)\n",
    "images_loader = DataLoader(images, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "optim_crit = optim.Adam(crit.parameters(), lr=LR, betas=(0.5, 0.9))\n",
    "optim_gen = optim.Adam(gen.parameters(), lr=LR, betas=(0.5, 0.9))\n",
    "\n",
    "writer_fake = SummaryWriter('logs/fake')\n",
    "writer_real = SummaryWriter('logs/real')\n",
    "steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Size: 1.429e+07, Discriminator Size: 2.804e+06\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Generator Size: {model_size(gen):.3e}, Discriminator Size: {model_size(crit):.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 -- Step: 1 -- Batch: 1/469 -- Crit Loss: -10.7217 -- Gen Loss: 9.2009\n",
      "Epoch: 1/50 -- Step: 2 -- Batch: 41/469 -- Crit Loss: -131.0640 -- Gen Loss: 110.4134\n",
      "Epoch: 1/50 -- Step: 3 -- Batch: 81/469 -- Crit Loss: -128.1404 -- Gen Loss: 120.5416\n",
      "Epoch: 1/50 -- Step: 4 -- Batch: 121/469 -- Crit Loss: -115.0725 -- Gen Loss: 109.5433\n",
      "Epoch: 1/50 -- Step: 5 -- Batch: 161/469 -- Crit Loss: -101.7197 -- Gen Loss: 99.7237\n",
      "Epoch: 1/50 -- Step: 6 -- Batch: 201/469 -- Crit Loss: -87.0332 -- Gen Loss: 95.6893\n",
      "Epoch: 1/50 -- Step: 7 -- Batch: 241/469 -- Crit Loss: -71.2586 -- Gen Loss: 90.8242\n",
      "Epoch: 1/50 -- Step: 8 -- Batch: 281/469 -- Crit Loss: -58.5242 -- Gen Loss: 92.8212\n",
      "Epoch: 1/50 -- Step: 9 -- Batch: 321/469 -- Crit Loss: -46.1799 -- Gen Loss: 90.5199\n",
      "Epoch: 1/50 -- Step: 10 -- Batch: 361/469 -- Crit Loss: -38.3794 -- Gen Loss: 94.3378\n",
      "Epoch: 1/50 -- Step: 11 -- Batch: 401/469 -- Crit Loss: -31.8554 -- Gen Loss: 97.4765\n",
      "Epoch: 1/50 -- Step: 12 -- Batch: 441/469 -- Crit Loss: -26.9273 -- Gen Loss: 97.4099\n",
      "Epoch: 2/50 -- Step: 13 -- Batch: 1/469 -- Crit Loss: -23.4525 -- Gen Loss: 100.9446\n",
      "Epoch: 2/50 -- Step: 14 -- Batch: 41/469 -- Crit Loss: -20.9741 -- Gen Loss: 101.4887\n",
      "Epoch: 2/50 -- Step: 15 -- Batch: 81/469 -- Crit Loss: -18.7156 -- Gen Loss: 98.0329\n",
      "Epoch: 2/50 -- Step: 16 -- Batch: 121/469 -- Crit Loss: -17.4448 -- Gen Loss: 102.0380\n",
      "Epoch: 2/50 -- Step: 17 -- Batch: 161/469 -- Crit Loss: -14.6433 -- Gen Loss: 100.9274\n",
      "Epoch: 2/50 -- Step: 18 -- Batch: 201/469 -- Crit Loss: -12.8423 -- Gen Loss: 102.6276\n",
      "Epoch: 2/50 -- Step: 19 -- Batch: 241/469 -- Crit Loss: -10.2939 -- Gen Loss: 100.7946\n",
      "Epoch: 2/50 -- Step: 20 -- Batch: 281/469 -- Crit Loss: -10.1432 -- Gen Loss: 98.4631\n",
      "Epoch: 2/50 -- Step: 21 -- Batch: 321/469 -- Crit Loss: -9.2701 -- Gen Loss: 96.1457\n",
      "Epoch: 2/50 -- Step: 22 -- Batch: 361/469 -- Crit Loss: -11.1954 -- Gen Loss: 91.5021\n",
      "Epoch: 2/50 -- Step: 23 -- Batch: 401/469 -- Crit Loss: -9.0332 -- Gen Loss: 91.7780\n",
      "Epoch: 2/50 -- Step: 24 -- Batch: 441/469 -- Crit Loss: -10.1409 -- Gen Loss: 93.3434\n",
      "Epoch: 3/50 -- Step: 25 -- Batch: 1/469 -- Crit Loss: -9.3795 -- Gen Loss: 89.7757\n",
      "Epoch: 3/50 -- Step: 26 -- Batch: 41/469 -- Crit Loss: -8.2340 -- Gen Loss: 91.3295\n",
      "Epoch: 3/50 -- Step: 27 -- Batch: 81/469 -- Crit Loss: -8.6340 -- Gen Loss: 92.2483\n",
      "Epoch: 3/50 -- Step: 28 -- Batch: 121/469 -- Crit Loss: -8.1836 -- Gen Loss: 94.2122\n",
      "Epoch: 3/50 -- Step: 29 -- Batch: 161/469 -- Crit Loss: -8.3012 -- Gen Loss: 94.5132\n",
      "Epoch: 3/50 -- Step: 30 -- Batch: 201/469 -- Crit Loss: -7.7328 -- Gen Loss: 90.9649\n",
      "Epoch: 3/50 -- Step: 31 -- Batch: 241/469 -- Crit Loss: -7.1704 -- Gen Loss: 90.8793\n",
      "Epoch: 3/50 -- Step: 32 -- Batch: 281/469 -- Crit Loss: -9.9108 -- Gen Loss: 94.8030\n",
      "Epoch: 3/50 -- Step: 33 -- Batch: 321/469 -- Crit Loss: -8.1101 -- Gen Loss: 94.1705\n",
      "Epoch: 3/50 -- Step: 34 -- Batch: 361/469 -- Crit Loss: -7.1360 -- Gen Loss: 89.4259\n",
      "Epoch: 3/50 -- Step: 35 -- Batch: 401/469 -- Crit Loss: -7.1097 -- Gen Loss: 96.3578\n",
      "Epoch: 3/50 -- Step: 36 -- Batch: 441/469 -- Crit Loss: -7.0806 -- Gen Loss: 95.1185\n",
      "Epoch: 4/50 -- Step: 37 -- Batch: 1/469 -- Crit Loss: -7.0162 -- Gen Loss: 92.8959\n",
      "Epoch: 4/50 -- Step: 38 -- Batch: 41/469 -- Crit Loss: -5.8293 -- Gen Loss: 96.7326\n",
      "Epoch: 4/50 -- Step: 39 -- Batch: 81/469 -- Crit Loss: -6.8894 -- Gen Loss: 97.0581\n",
      "Epoch: 4/50 -- Step: 40 -- Batch: 121/469 -- Crit Loss: -6.8340 -- Gen Loss: 95.2982\n",
      "Epoch: 4/50 -- Step: 41 -- Batch: 161/469 -- Crit Loss: -5.8940 -- Gen Loss: 91.7745\n",
      "Epoch: 4/50 -- Step: 42 -- Batch: 201/469 -- Crit Loss: -7.2794 -- Gen Loss: 93.8029\n",
      "Epoch: 4/50 -- Step: 43 -- Batch: 241/469 -- Crit Loss: -5.7676 -- Gen Loss: 97.2189\n",
      "Epoch: 4/50 -- Step: 44 -- Batch: 281/469 -- Crit Loss: -5.6699 -- Gen Loss: 92.6717\n",
      "Epoch: 4/50 -- Step: 45 -- Batch: 321/469 -- Crit Loss: -7.9040 -- Gen Loss: 92.3923\n",
      "Epoch: 4/50 -- Step: 46 -- Batch: 361/469 -- Crit Loss: -7.1738 -- Gen Loss: 89.2658\n",
      "Epoch: 4/50 -- Step: 47 -- Batch: 401/469 -- Crit Loss: -6.9661 -- Gen Loss: 90.5471\n",
      "Epoch: 4/50 -- Step: 48 -- Batch: 441/469 -- Crit Loss: -5.7486 -- Gen Loss: 89.1104\n",
      "Epoch: 5/50 -- Step: 49 -- Batch: 1/469 -- Crit Loss: -7.9577 -- Gen Loss: 92.7891\n",
      "Epoch: 5/50 -- Step: 50 -- Batch: 41/469 -- Crit Loss: -5.6177 -- Gen Loss: 88.9654\n",
      "Epoch: 5/50 -- Step: 51 -- Batch: 81/469 -- Crit Loss: -7.1467 -- Gen Loss: 91.4926\n",
      "Epoch: 5/50 -- Step: 52 -- Batch: 121/469 -- Crit Loss: -6.7823 -- Gen Loss: 93.0308\n",
      "Epoch: 5/50 -- Step: 53 -- Batch: 161/469 -- Crit Loss: -6.9756 -- Gen Loss: 89.1136\n",
      "Epoch: 5/50 -- Step: 54 -- Batch: 201/469 -- Crit Loss: -5.0939 -- Gen Loss: 86.0466\n",
      "Epoch: 5/50 -- Step: 55 -- Batch: 241/469 -- Crit Loss: -6.0961 -- Gen Loss: 84.9874\n",
      "Epoch: 5/50 -- Step: 56 -- Batch: 281/469 -- Crit Loss: -5.5682 -- Gen Loss: 83.1018\n",
      "Epoch: 5/50 -- Step: 57 -- Batch: 321/469 -- Crit Loss: -5.9044 -- Gen Loss: 86.4581\n",
      "Epoch: 5/50 -- Step: 58 -- Batch: 361/469 -- Crit Loss: -5.5003 -- Gen Loss: 88.5021\n",
      "Epoch: 5/50 -- Step: 59 -- Batch: 401/469 -- Crit Loss: -5.1937 -- Gen Loss: 85.7946\n",
      "Epoch: 5/50 -- Step: 60 -- Batch: 441/469 -- Crit Loss: -5.0929 -- Gen Loss: 87.7391\n",
      "Epoch: 6/50 -- Step: 61 -- Batch: 1/469 -- Crit Loss: -5.4798 -- Gen Loss: 90.1508\n",
      "Epoch: 6/50 -- Step: 62 -- Batch: 41/469 -- Crit Loss: -5.5422 -- Gen Loss: 91.8326\n",
      "Epoch: 6/50 -- Step: 63 -- Batch: 81/469 -- Crit Loss: -6.2642 -- Gen Loss: 88.9417\n",
      "Epoch: 6/50 -- Step: 64 -- Batch: 121/469 -- Crit Loss: -5.2288 -- Gen Loss: 85.0467\n",
      "Epoch: 6/50 -- Step: 65 -- Batch: 161/469 -- Crit Loss: -6.0696 -- Gen Loss: 86.8430\n",
      "Epoch: 6/50 -- Step: 66 -- Batch: 201/469 -- Crit Loss: -5.4840 -- Gen Loss: 86.0988\n",
      "Epoch: 6/50 -- Step: 67 -- Batch: 241/469 -- Crit Loss: -4.8553 -- Gen Loss: 88.3932\n",
      "Epoch: 6/50 -- Step: 68 -- Batch: 281/469 -- Crit Loss: -5.5675 -- Gen Loss: 85.6749\n",
      "Epoch: 6/50 -- Step: 69 -- Batch: 321/469 -- Crit Loss: -5.9094 -- Gen Loss: 81.8830\n",
      "Epoch: 6/50 -- Step: 70 -- Batch: 361/469 -- Crit Loss: -6.4569 -- Gen Loss: 86.2266\n",
      "Epoch: 6/50 -- Step: 71 -- Batch: 401/469 -- Crit Loss: -4.3900 -- Gen Loss: 85.5817\n",
      "Epoch: 6/50 -- Step: 72 -- Batch: 441/469 -- Crit Loss: -5.6447 -- Gen Loss: 79.6236\n",
      "Epoch: 7/50 -- Step: 73 -- Batch: 1/469 -- Crit Loss: -5.5006 -- Gen Loss: 85.5040\n",
      "Epoch: 7/50 -- Step: 74 -- Batch: 41/469 -- Crit Loss: -5.6223 -- Gen Loss: 83.0981\n",
      "Epoch: 7/50 -- Step: 75 -- Batch: 81/469 -- Crit Loss: -4.9115 -- Gen Loss: 84.1450\n",
      "Epoch: 7/50 -- Step: 76 -- Batch: 121/469 -- Crit Loss: -5.1879 -- Gen Loss: 78.8162\n",
      "Epoch: 7/50 -- Step: 77 -- Batch: 161/469 -- Crit Loss: -4.9833 -- Gen Loss: 81.9830\n",
      "Epoch: 7/50 -- Step: 78 -- Batch: 201/469 -- Crit Loss: -5.3224 -- Gen Loss: 81.4746\n",
      "Epoch: 7/50 -- Step: 79 -- Batch: 241/469 -- Crit Loss: -5.2902 -- Gen Loss: 84.5475\n",
      "Epoch: 7/50 -- Step: 80 -- Batch: 281/469 -- Crit Loss: -4.5543 -- Gen Loss: 84.5313\n",
      "Epoch: 7/50 -- Step: 81 -- Batch: 321/469 -- Crit Loss: -5.3192 -- Gen Loss: 80.4156\n",
      "Epoch: 7/50 -- Step: 82 -- Batch: 361/469 -- Crit Loss: -4.1767 -- Gen Loss: 79.2270\n",
      "Epoch: 7/50 -- Step: 83 -- Batch: 401/469 -- Crit Loss: -4.6534 -- Gen Loss: 81.6392\n",
      "Epoch: 7/50 -- Step: 84 -- Batch: 441/469 -- Crit Loss: -4.0158 -- Gen Loss: 80.8413\n",
      "Epoch: 8/50 -- Step: 85 -- Batch: 1/469 -- Crit Loss: -4.8814 -- Gen Loss: 68.9469\n",
      "Epoch: 8/50 -- Step: 86 -- Batch: 41/469 -- Crit Loss: -6.0457 -- Gen Loss: 82.3920\n",
      "Epoch: 8/50 -- Step: 87 -- Batch: 81/469 -- Crit Loss: -4.9843 -- Gen Loss: 80.5983\n",
      "Epoch: 8/50 -- Step: 88 -- Batch: 121/469 -- Crit Loss: -4.7978 -- Gen Loss: 76.4752\n",
      "Epoch: 8/50 -- Step: 89 -- Batch: 161/469 -- Crit Loss: -3.8934 -- Gen Loss: 80.9730\n",
      "Epoch: 8/50 -- Step: 90 -- Batch: 201/469 -- Crit Loss: -4.5374 -- Gen Loss: 81.0291\n",
      "Epoch: 8/50 -- Step: 91 -- Batch: 241/469 -- Crit Loss: -2.5417 -- Gen Loss: 73.4416\n",
      "Epoch: 8/50 -- Step: 92 -- Batch: 281/469 -- Crit Loss: -4.8103 -- Gen Loss: 74.8441\n",
      "Epoch: 8/50 -- Step: 93 -- Batch: 321/469 -- Crit Loss: -4.6071 -- Gen Loss: 79.8933\n",
      "Epoch: 8/50 -- Step: 94 -- Batch: 361/469 -- Crit Loss: -3.4163 -- Gen Loss: 75.4710\n",
      "Epoch: 8/50 -- Step: 95 -- Batch: 401/469 -- Crit Loss: -4.9148 -- Gen Loss: 74.8232\n",
      "Epoch: 8/50 -- Step: 96 -- Batch: 441/469 -- Crit Loss: -3.6486 -- Gen Loss: 77.6088\n",
      "Epoch: 9/50 -- Step: 97 -- Batch: 1/469 -- Crit Loss: -4.9665 -- Gen Loss: 72.6216\n",
      "Epoch: 9/50 -- Step: 98 -- Batch: 41/469 -- Crit Loss: -4.1947 -- Gen Loss: 72.1295\n",
      "Epoch: 9/50 -- Step: 99 -- Batch: 81/469 -- Crit Loss: -3.7966 -- Gen Loss: 67.6101\n",
      "Epoch: 9/50 -- Step: 100 -- Batch: 121/469 -- Crit Loss: -5.2418 -- Gen Loss: 65.6142\n",
      "Epoch: 9/50 -- Step: 101 -- Batch: 161/469 -- Crit Loss: -4.2415 -- Gen Loss: 71.4472\n",
      "Epoch: 9/50 -- Step: 102 -- Batch: 201/469 -- Crit Loss: -5.2925 -- Gen Loss: 67.0315\n",
      "Epoch: 9/50 -- Step: 103 -- Batch: 241/469 -- Crit Loss: -4.2576 -- Gen Loss: 69.9922\n",
      "Epoch: 9/50 -- Step: 104 -- Batch: 281/469 -- Crit Loss: -2.8817 -- Gen Loss: 64.7146\n",
      "Epoch: 9/50 -- Step: 105 -- Batch: 321/469 -- Crit Loss: -5.8481 -- Gen Loss: 70.7151\n",
      "Epoch: 9/50 -- Step: 106 -- Batch: 361/469 -- Crit Loss: -4.6623 -- Gen Loss: 73.2692\n",
      "Epoch: 9/50 -- Step: 107 -- Batch: 401/469 -- Crit Loss: -5.7554 -- Gen Loss: 67.4691\n",
      "Epoch: 9/50 -- Step: 108 -- Batch: 441/469 -- Crit Loss: -4.2809 -- Gen Loss: 63.0168\n",
      "Epoch: 10/50 -- Step: 109 -- Batch: 1/469 -- Crit Loss: -4.7903 -- Gen Loss: 61.3234\n",
      "Epoch: 10/50 -- Step: 110 -- Batch: 41/469 -- Crit Loss: -4.3722 -- Gen Loss: 72.0867\n",
      "Epoch: 10/50 -- Step: 111 -- Batch: 81/469 -- Crit Loss: -4.4949 -- Gen Loss: 69.6339\n",
      "Epoch: 10/50 -- Step: 112 -- Batch: 121/469 -- Crit Loss: -4.2990 -- Gen Loss: 64.8615\n",
      "Epoch: 10/50 -- Step: 113 -- Batch: 161/469 -- Crit Loss: -5.1111 -- Gen Loss: 64.2667\n",
      "Epoch: 10/50 -- Step: 114 -- Batch: 201/469 -- Crit Loss: -4.2265 -- Gen Loss: 60.2088\n",
      "Epoch: 10/50 -- Step: 115 -- Batch: 241/469 -- Crit Loss: -5.0909 -- Gen Loss: 60.2487\n",
      "Epoch: 10/50 -- Step: 116 -- Batch: 281/469 -- Crit Loss: -4.2825 -- Gen Loss: 62.7082\n",
      "Epoch: 10/50 -- Step: 117 -- Batch: 321/469 -- Crit Loss: -4.0769 -- Gen Loss: 64.0765\n",
      "Epoch: 10/50 -- Step: 118 -- Batch: 361/469 -- Crit Loss: -4.7450 -- Gen Loss: 63.5621\n",
      "Epoch: 10/50 -- Step: 119 -- Batch: 401/469 -- Crit Loss: -4.2711 -- Gen Loss: 63.5244\n",
      "Epoch: 10/50 -- Step: 120 -- Batch: 441/469 -- Crit Loss: -3.6278 -- Gen Loss: 67.3241\n",
      "Epoch: 11/50 -- Step: 121 -- Batch: 1/469 -- Crit Loss: -4.6871 -- Gen Loss: 64.3994\n",
      "Epoch: 11/50 -- Step: 122 -- Batch: 41/469 -- Crit Loss: -4.4150 -- Gen Loss: 61.4269\n",
      "Epoch: 11/50 -- Step: 123 -- Batch: 81/469 -- Crit Loss: -5.9115 -- Gen Loss: 55.4380\n",
      "Epoch: 11/50 -- Step: 124 -- Batch: 121/469 -- Crit Loss: -3.1033 -- Gen Loss: 53.2775\n",
      "Epoch: 11/50 -- Step: 125 -- Batch: 161/469 -- Crit Loss: -3.5339 -- Gen Loss: 61.4553\n",
      "Epoch: 11/50 -- Step: 126 -- Batch: 201/469 -- Crit Loss: -2.7526 -- Gen Loss: 62.5862\n",
      "Epoch: 11/50 -- Step: 127 -- Batch: 241/469 -- Crit Loss: -3.3355 -- Gen Loss: 54.3936\n",
      "Epoch: 11/50 -- Step: 128 -- Batch: 281/469 -- Crit Loss: -3.7541 -- Gen Loss: 55.4871\n",
      "Epoch: 11/50 -- Step: 129 -- Batch: 321/469 -- Crit Loss: -3.6415 -- Gen Loss: 56.4025\n",
      "Epoch: 11/50 -- Step: 130 -- Batch: 361/469 -- Crit Loss: -3.5010 -- Gen Loss: 60.0023\n",
      "Epoch: 11/50 -- Step: 131 -- Batch: 401/469 -- Crit Loss: -4.6822 -- Gen Loss: 56.8684\n",
      "Epoch: 11/50 -- Step: 132 -- Batch: 441/469 -- Crit Loss: -6.7833 -- Gen Loss: 45.7644\n",
      "Epoch: 12/50 -- Step: 133 -- Batch: 1/469 -- Crit Loss: -3.3660 -- Gen Loss: 45.1450\n",
      "Epoch: 12/50 -- Step: 134 -- Batch: 41/469 -- Crit Loss: -4.0437 -- Gen Loss: 51.1507\n",
      "Epoch: 12/50 -- Step: 135 -- Batch: 81/469 -- Crit Loss: -3.5008 -- Gen Loss: 57.5120\n",
      "Epoch: 12/50 -- Step: 136 -- Batch: 121/469 -- Crit Loss: -4.5748 -- Gen Loss: 53.3190\n",
      "Epoch: 12/50 -- Step: 137 -- Batch: 161/469 -- Crit Loss: -4.4495 -- Gen Loss: 45.4724\n",
      "Epoch: 12/50 -- Step: 138 -- Batch: 201/469 -- Crit Loss: -3.7076 -- Gen Loss: 51.2999\n",
      "Epoch: 12/50 -- Step: 139 -- Batch: 241/469 -- Crit Loss: -4.9540 -- Gen Loss: 53.9529\n",
      "Epoch: 12/50 -- Step: 140 -- Batch: 281/469 -- Crit Loss: -4.5364 -- Gen Loss: 50.0297\n",
      "Epoch: 12/50 -- Step: 141 -- Batch: 321/469 -- Crit Loss: -2.4968 -- Gen Loss: 45.5914\n",
      "Epoch: 12/50 -- Step: 142 -- Batch: 361/469 -- Crit Loss: -3.2291 -- Gen Loss: 50.6654\n",
      "Epoch: 12/50 -- Step: 143 -- Batch: 401/469 -- Crit Loss: -2.5422 -- Gen Loss: 50.5654\n",
      "Epoch: 12/50 -- Step: 144 -- Batch: 441/469 -- Crit Loss: -3.2343 -- Gen Loss: 56.2949\n",
      "Epoch: 13/50 -- Step: 145 -- Batch: 1/469 -- Crit Loss: -3.3025 -- Gen Loss: 50.9771\n",
      "Epoch: 13/50 -- Step: 146 -- Batch: 41/469 -- Crit Loss: -4.5371 -- Gen Loss: 50.5588\n",
      "Epoch: 13/50 -- Step: 147 -- Batch: 81/469 -- Crit Loss: -3.7603 -- Gen Loss: 53.0862\n",
      "Epoch: 13/50 -- Step: 148 -- Batch: 121/469 -- Crit Loss: -3.8209 -- Gen Loss: 44.2963\n",
      "Epoch: 13/50 -- Step: 149 -- Batch: 161/469 -- Crit Loss: -4.2627 -- Gen Loss: 45.3484\n",
      "Epoch: 13/50 -- Step: 150 -- Batch: 201/469 -- Crit Loss: -3.6581 -- Gen Loss: 43.0833\n",
      "Epoch: 13/50 -- Step: 151 -- Batch: 241/469 -- Crit Loss: -3.2561 -- Gen Loss: 49.3249\n",
      "Epoch: 13/50 -- Step: 152 -- Batch: 281/469 -- Crit Loss: -3.3061 -- Gen Loss: 44.8154\n",
      "Epoch: 13/50 -- Step: 153 -- Batch: 321/469 -- Crit Loss: -4.7760 -- Gen Loss: 39.1901\n",
      "Epoch: 13/50 -- Step: 154 -- Batch: 361/469 -- Crit Loss: -3.8383 -- Gen Loss: 43.9474\n",
      "Epoch: 13/50 -- Step: 155 -- Batch: 401/469 -- Crit Loss: -5.2119 -- Gen Loss: 43.3993\n",
      "Epoch: 13/50 -- Step: 156 -- Batch: 441/469 -- Crit Loss: -3.9576 -- Gen Loss: 47.1857\n",
      "Epoch: 14/50 -- Step: 157 -- Batch: 1/469 -- Crit Loss: -4.0780 -- Gen Loss: 44.5554\n",
      "Epoch: 14/50 -- Step: 158 -- Batch: 41/469 -- Crit Loss: -3.7187 -- Gen Loss: 44.5301\n",
      "Epoch: 14/50 -- Step: 159 -- Batch: 81/469 -- Crit Loss: -3.3093 -- Gen Loss: 40.4917\n",
      "Epoch: 14/50 -- Step: 160 -- Batch: 121/469 -- Crit Loss: -4.2078 -- Gen Loss: 44.4844\n",
      "Epoch: 14/50 -- Step: 161 -- Batch: 161/469 -- Crit Loss: -3.2652 -- Gen Loss: 41.3006\n",
      "Epoch: 14/50 -- Step: 162 -- Batch: 201/469 -- Crit Loss: -4.2312 -- Gen Loss: 44.7317\n",
      "Epoch: 14/50 -- Step: 163 -- Batch: 241/469 -- Crit Loss: -4.1737 -- Gen Loss: 43.2985\n",
      "Epoch: 14/50 -- Step: 164 -- Batch: 281/469 -- Crit Loss: -3.8724 -- Gen Loss: 40.8407\n",
      "Epoch: 14/50 -- Step: 165 -- Batch: 321/469 -- Crit Loss: -4.2517 -- Gen Loss: 39.3600\n",
      "Epoch: 14/50 -- Step: 166 -- Batch: 361/469 -- Crit Loss: -3.0430 -- Gen Loss: 40.5497\n",
      "Epoch: 14/50 -- Step: 167 -- Batch: 401/469 -- Crit Loss: -3.2883 -- Gen Loss: 35.5797\n",
      "Epoch: 14/50 -- Step: 168 -- Batch: 441/469 -- Crit Loss: -3.8657 -- Gen Loss: 33.2130\n",
      "Epoch: 15/50 -- Step: 169 -- Batch: 1/469 -- Crit Loss: -3.3850 -- Gen Loss: 42.4719\n",
      "Epoch: 15/50 -- Step: 170 -- Batch: 41/469 -- Crit Loss: -3.0709 -- Gen Loss: 41.1199\n",
      "Epoch: 15/50 -- Step: 171 -- Batch: 81/469 -- Crit Loss: -4.3580 -- Gen Loss: 36.6328\n",
      "Epoch: 15/50 -- Step: 172 -- Batch: 121/469 -- Crit Loss: -5.0698 -- Gen Loss: 44.7091\n",
      "Epoch: 15/50 -- Step: 173 -- Batch: 161/469 -- Crit Loss: -3.2320 -- Gen Loss: 40.3090\n",
      "Epoch: 15/50 -- Step: 174 -- Batch: 201/469 -- Crit Loss: -5.2835 -- Gen Loss: 35.4106\n",
      "Epoch: 15/50 -- Step: 175 -- Batch: 241/469 -- Crit Loss: -3.8954 -- Gen Loss: 32.4206\n",
      "Epoch: 15/50 -- Step: 176 -- Batch: 281/469 -- Crit Loss: -2.5182 -- Gen Loss: 37.9280\n",
      "Epoch: 15/50 -- Step: 177 -- Batch: 321/469 -- Crit Loss: -2.8610 -- Gen Loss: 27.2940\n",
      "Epoch: 15/50 -- Step: 178 -- Batch: 361/469 -- Crit Loss: -3.7914 -- Gen Loss: 37.5844\n",
      "Epoch: 15/50 -- Step: 179 -- Batch: 401/469 -- Crit Loss: -2.8378 -- Gen Loss: 31.8157\n",
      "Epoch: 15/50 -- Step: 180 -- Batch: 441/469 -- Crit Loss: -2.0343 -- Gen Loss: 28.2495\n",
      "Epoch: 16/50 -- Step: 181 -- Batch: 1/469 -- Crit Loss: -2.1914 -- Gen Loss: 30.5086\n",
      "Epoch: 16/50 -- Step: 182 -- Batch: 41/469 -- Crit Loss: -4.0768 -- Gen Loss: 26.6933\n",
      "Epoch: 16/50 -- Step: 183 -- Batch: 81/469 -- Crit Loss: -2.1586 -- Gen Loss: 25.1117\n",
      "Epoch: 16/50 -- Step: 184 -- Batch: 121/469 -- Crit Loss: -3.8676 -- Gen Loss: 30.1580\n",
      "Epoch: 16/50 -- Step: 185 -- Batch: 161/469 -- Crit Loss: -2.5339 -- Gen Loss: 40.6139\n",
      "Epoch: 16/50 -- Step: 186 -- Batch: 201/469 -- Crit Loss: -3.6125 -- Gen Loss: 37.9996\n",
      "Epoch: 16/50 -- Step: 187 -- Batch: 241/469 -- Crit Loss: -4.1404 -- Gen Loss: 30.4247\n",
      "Epoch: 16/50 -- Step: 188 -- Batch: 281/469 -- Crit Loss: -3.8994 -- Gen Loss: 32.6817\n",
      "Epoch: 16/50 -- Step: 189 -- Batch: 321/469 -- Crit Loss: -3.1751 -- Gen Loss: 25.3102\n",
      "Epoch: 16/50 -- Step: 190 -- Batch: 361/469 -- Crit Loss: -3.3338 -- Gen Loss: 28.1273\n",
      "Epoch: 16/50 -- Step: 191 -- Batch: 401/469 -- Crit Loss: -3.6744 -- Gen Loss: 31.9102\n",
      "Epoch: 16/50 -- Step: 192 -- Batch: 441/469 -- Crit Loss: -3.4010 -- Gen Loss: 26.7433\n",
      "Epoch: 17/50 -- Step: 193 -- Batch: 1/469 -- Crit Loss: -4.0943 -- Gen Loss: 30.5860\n",
      "Epoch: 17/50 -- Step: 194 -- Batch: 41/469 -- Crit Loss: -3.8303 -- Gen Loss: 29.0147\n",
      "Epoch: 17/50 -- Step: 195 -- Batch: 81/469 -- Crit Loss: -2.6962 -- Gen Loss: 24.5254\n",
      "Epoch: 17/50 -- Step: 196 -- Batch: 121/469 -- Crit Loss: -2.8799 -- Gen Loss: 34.2530\n",
      "Epoch: 17/50 -- Step: 197 -- Batch: 161/469 -- Crit Loss: -4.2137 -- Gen Loss: 22.8152\n",
      "Epoch: 17/50 -- Step: 198 -- Batch: 201/469 -- Crit Loss: -3.4840 -- Gen Loss: 19.1667\n",
      "Epoch: 17/50 -- Step: 199 -- Batch: 241/469 -- Crit Loss: -3.0009 -- Gen Loss: 21.9556\n",
      "Epoch: 17/50 -- Step: 200 -- Batch: 281/469 -- Crit Loss: -2.7344 -- Gen Loss: 21.2723\n",
      "Epoch: 17/50 -- Step: 201 -- Batch: 321/469 -- Crit Loss: -3.4545 -- Gen Loss: 20.5742\n",
      "Epoch: 17/50 -- Step: 202 -- Batch: 361/469 -- Crit Loss: -3.4234 -- Gen Loss: 23.3965\n",
      "Epoch: 17/50 -- Step: 203 -- Batch: 401/469 -- Crit Loss: -3.6129 -- Gen Loss: 24.3740\n",
      "Epoch: 17/50 -- Step: 204 -- Batch: 441/469 -- Crit Loss: -2.2586 -- Gen Loss: 21.3539\n",
      "Epoch: 18/50 -- Step: 205 -- Batch: 1/469 -- Crit Loss: -3.9405 -- Gen Loss: 24.9327\n",
      "Epoch: 18/50 -- Step: 206 -- Batch: 41/469 -- Crit Loss: -4.1740 -- Gen Loss: 21.9788\n",
      "Epoch: 18/50 -- Step: 207 -- Batch: 81/469 -- Crit Loss: -3.0605 -- Gen Loss: 18.6613\n",
      "Epoch: 18/50 -- Step: 208 -- Batch: 121/469 -- Crit Loss: -4.5427 -- Gen Loss: 19.4637\n",
      "Epoch: 18/50 -- Step: 209 -- Batch: 161/469 -- Crit Loss: -3.2454 -- Gen Loss: 17.1284\n",
      "Epoch: 18/50 -- Step: 210 -- Batch: 201/469 -- Crit Loss: -3.6794 -- Gen Loss: 20.8162\n",
      "Epoch: 18/50 -- Step: 211 -- Batch: 241/469 -- Crit Loss: -2.5012 -- Gen Loss: 22.1294\n",
      "Epoch: 18/50 -- Step: 212 -- Batch: 281/469 -- Crit Loss: -2.7097 -- Gen Loss: 17.6346\n",
      "Epoch: 18/50 -- Step: 213 -- Batch: 321/469 -- Crit Loss: -3.2318 -- Gen Loss: 9.4708\n",
      "Epoch: 18/50 -- Step: 214 -- Batch: 361/469 -- Crit Loss: -3.0581 -- Gen Loss: 15.4891\n",
      "Epoch: 18/50 -- Step: 215 -- Batch: 401/469 -- Crit Loss: -2.8296 -- Gen Loss: 13.5036\n",
      "Epoch: 18/50 -- Step: 216 -- Batch: 441/469 -- Crit Loss: -3.9857 -- Gen Loss: 16.1601\n",
      "Epoch: 19/50 -- Step: 217 -- Batch: 1/469 -- Crit Loss: -2.3683 -- Gen Loss: 19.8553\n",
      "Epoch: 19/50 -- Step: 218 -- Batch: 41/469 -- Crit Loss: -2.8726 -- Gen Loss: 18.2606\n",
      "Epoch: 19/50 -- Step: 219 -- Batch: 81/469 -- Crit Loss: -3.6652 -- Gen Loss: 10.2901\n",
      "Epoch: 19/50 -- Step: 220 -- Batch: 121/469 -- Crit Loss: -4.0987 -- Gen Loss: 10.9632\n",
      "Epoch: 19/50 -- Step: 221 -- Batch: 161/469 -- Crit Loss: -2.7213 -- Gen Loss: 13.3846\n",
      "Epoch: 19/50 -- Step: 222 -- Batch: 201/469 -- Crit Loss: -2.7836 -- Gen Loss: 15.8380\n",
      "Epoch: 19/50 -- Step: 223 -- Batch: 241/469 -- Crit Loss: -3.7622 -- Gen Loss: 3.6376\n",
      "Epoch: 19/50 -- Step: 224 -- Batch: 281/469 -- Crit Loss: -2.3507 -- Gen Loss: 7.1410\n",
      "Epoch: 19/50 -- Step: 225 -- Batch: 321/469 -- Crit Loss: -3.2424 -- Gen Loss: 14.4277\n",
      "Epoch: 19/50 -- Step: 226 -- Batch: 361/469 -- Crit Loss: -3.1723 -- Gen Loss: 7.3710\n",
      "Epoch: 19/50 -- Step: 227 -- Batch: 401/469 -- Crit Loss: -3.2378 -- Gen Loss: 10.2338\n",
      "Epoch: 19/50 -- Step: 228 -- Batch: 441/469 -- Crit Loss: -3.4008 -- Gen Loss: 5.9398\n",
      "Epoch: 20/50 -- Step: 229 -- Batch: 1/469 -- Crit Loss: -2.1665 -- Gen Loss: 11.4864\n",
      "Epoch: 20/50 -- Step: 230 -- Batch: 41/469 -- Crit Loss: -3.4828 -- Gen Loss: 11.0880\n",
      "Epoch: 20/50 -- Step: 231 -- Batch: 81/469 -- Crit Loss: -2.8561 -- Gen Loss: 6.4563\n",
      "Epoch: 20/50 -- Step: 232 -- Batch: 121/469 -- Crit Loss: -2.9334 -- Gen Loss: 3.8231\n",
      "Epoch: 20/50 -- Step: 233 -- Batch: 161/469 -- Crit Loss: -2.5597 -- Gen Loss: 12.2567\n",
      "Epoch: 20/50 -- Step: 234 -- Batch: 201/469 -- Crit Loss: -3.0237 -- Gen Loss: 3.8676\n",
      "Epoch: 20/50 -- Step: 235 -- Batch: 241/469 -- Crit Loss: -1.3126 -- Gen Loss: 5.9967\n",
      "Epoch: 20/50 -- Step: 236 -- Batch: 281/469 -- Crit Loss: -3.0031 -- Gen Loss: 2.4518\n",
      "Epoch: 20/50 -- Step: 237 -- Batch: 321/469 -- Crit Loss: -1.4606 -- Gen Loss: 5.8332\n",
      "Epoch: 20/50 -- Step: 238 -- Batch: 361/469 -- Crit Loss: -2.5611 -- Gen Loss: 9.8837\n",
      "Epoch: 20/50 -- Step: 239 -- Batch: 401/469 -- Crit Loss: -3.0342 -- Gen Loss: 2.7369\n",
      "Epoch: 20/50 -- Step: 240 -- Batch: 441/469 -- Crit Loss: -4.0963 -- Gen Loss: 6.3638\n",
      "Epoch: 21/50 -- Step: 241 -- Batch: 1/469 -- Crit Loss: -1.7364 -- Gen Loss: -1.6567\n",
      "Epoch: 21/50 -- Step: 242 -- Batch: 41/469 -- Crit Loss: -3.1971 -- Gen Loss: 1.3589\n",
      "Epoch: 21/50 -- Step: 243 -- Batch: 81/469 -- Crit Loss: -2.5067 -- Gen Loss: 10.6061\n",
      "Epoch: 21/50 -- Step: 244 -- Batch: 121/469 -- Crit Loss: -1.7101 -- Gen Loss: 3.0077\n",
      "Epoch: 21/50 -- Step: 245 -- Batch: 161/469 -- Crit Loss: -4.2844 -- Gen Loss: -2.8584\n",
      "Epoch: 21/50 -- Step: 246 -- Batch: 201/469 -- Crit Loss: -4.1136 -- Gen Loss: -1.0635\n",
      "Epoch: 21/50 -- Step: 247 -- Batch: 241/469 -- Crit Loss: -4.1621 -- Gen Loss: -0.6191\n",
      "Epoch: 21/50 -- Step: 248 -- Batch: 281/469 -- Crit Loss: -3.6776 -- Gen Loss: -6.7768\n",
      "Epoch: 21/50 -- Step: 249 -- Batch: 321/469 -- Crit Loss: -4.9638 -- Gen Loss: 7.9491\n",
      "Epoch: 21/50 -- Step: 250 -- Batch: 361/469 -- Crit Loss: -3.1052 -- Gen Loss: -2.7620\n",
      "Epoch: 21/50 -- Step: 251 -- Batch: 401/469 -- Crit Loss: -3.0442 -- Gen Loss: -3.3817\n",
      "Epoch: 21/50 -- Step: 252 -- Batch: 441/469 -- Crit Loss: -1.5143 -- Gen Loss: -7.9965\n",
      "Epoch: 22/50 -- Step: 253 -- Batch: 1/469 -- Crit Loss: -1.9051 -- Gen Loss: -10.4471\n",
      "Epoch: 22/50 -- Step: 254 -- Batch: 41/469 -- Crit Loss: -2.1341 -- Gen Loss: -4.0757\n",
      "Epoch: 22/50 -- Step: 255 -- Batch: 81/469 -- Crit Loss: -3.6348 -- Gen Loss: -1.7842\n",
      "Epoch: 22/50 -- Step: 256 -- Batch: 121/469 -- Crit Loss: -2.3562 -- Gen Loss: -10.2872\n",
      "Epoch: 22/50 -- Step: 257 -- Batch: 161/469 -- Crit Loss: -2.8861 -- Gen Loss: -6.9236\n",
      "Epoch: 22/50 -- Step: 258 -- Batch: 201/469 -- Crit Loss: -2.9124 -- Gen Loss: -13.7860\n",
      "Epoch: 22/50 -- Step: 259 -- Batch: 241/469 -- Crit Loss: -4.0007 -- Gen Loss: -8.8134\n",
      "Epoch: 22/50 -- Step: 260 -- Batch: 281/469 -- Crit Loss: -3.0793 -- Gen Loss: -15.2164\n",
      "Epoch: 22/50 -- Step: 261 -- Batch: 321/469 -- Crit Loss: -3.5961 -- Gen Loss: -14.1280\n",
      "Epoch: 22/50 -- Step: 262 -- Batch: 361/469 -- Crit Loss: -2.8727 -- Gen Loss: -5.9329\n",
      "Epoch: 22/50 -- Step: 263 -- Batch: 401/469 -- Crit Loss: -4.2359 -- Gen Loss: -5.0502\n",
      "Epoch: 22/50 -- Step: 264 -- Batch: 441/469 -- Crit Loss: -2.3293 -- Gen Loss: -5.5185\n",
      "Epoch: 23/50 -- Step: 265 -- Batch: 1/469 -- Crit Loss: -5.4015 -- Gen Loss: -9.8537\n",
      "Epoch: 23/50 -- Step: 266 -- Batch: 41/469 -- Crit Loss: -1.5995 -- Gen Loss: -12.2347\n",
      "Epoch: 23/50 -- Step: 267 -- Batch: 81/469 -- Crit Loss: -3.6574 -- Gen Loss: -14.7764\n",
      "Epoch: 23/50 -- Step: 268 -- Batch: 121/469 -- Crit Loss: -2.7421 -- Gen Loss: -15.2560\n",
      "Epoch: 23/50 -- Step: 269 -- Batch: 161/469 -- Crit Loss: -1.1940 -- Gen Loss: -19.6280\n",
      "Epoch: 23/50 -- Step: 270 -- Batch: 201/469 -- Crit Loss: -2.5768 -- Gen Loss: -17.0909\n",
      "Epoch: 23/50 -- Step: 271 -- Batch: 241/469 -- Crit Loss: -2.6550 -- Gen Loss: -17.8028\n",
      "Epoch: 23/50 -- Step: 272 -- Batch: 281/469 -- Crit Loss: -1.9458 -- Gen Loss: -13.4144\n",
      "Epoch: 23/50 -- Step: 273 -- Batch: 321/469 -- Crit Loss: -2.5187 -- Gen Loss: -11.5828\n",
      "Epoch: 23/50 -- Step: 274 -- Batch: 361/469 -- Crit Loss: -2.8449 -- Gen Loss: -8.8637\n",
      "Epoch: 23/50 -- Step: 275 -- Batch: 401/469 -- Crit Loss: -2.4746 -- Gen Loss: -19.4226\n",
      "Epoch: 23/50 -- Step: 276 -- Batch: 441/469 -- Crit Loss: -3.2748 -- Gen Loss: -21.8937\n",
      "Epoch: 24/50 -- Step: 277 -- Batch: 1/469 -- Crit Loss: -2.2714 -- Gen Loss: -26.0074\n",
      "Epoch: 24/50 -- Step: 278 -- Batch: 41/469 -- Crit Loss: -3.4936 -- Gen Loss: -25.0442\n",
      "Epoch: 24/50 -- Step: 279 -- Batch: 81/469 -- Crit Loss: -3.4749 -- Gen Loss: -9.9164\n",
      "Epoch: 24/50 -- Step: 280 -- Batch: 121/469 -- Crit Loss: -2.6561 -- Gen Loss: -21.3120\n",
      "Epoch: 24/50 -- Step: 281 -- Batch: 161/469 -- Crit Loss: -3.7169 -- Gen Loss: -17.7162\n",
      "Epoch: 24/50 -- Step: 282 -- Batch: 201/469 -- Crit Loss: -2.3694 -- Gen Loss: -19.5081\n",
      "Epoch: 24/50 -- Step: 283 -- Batch: 241/469 -- Crit Loss: -3.0939 -- Gen Loss: -29.6747\n",
      "Epoch: 24/50 -- Step: 284 -- Batch: 281/469 -- Crit Loss: -3.0227 -- Gen Loss: -29.9328\n",
      "Epoch: 24/50 -- Step: 285 -- Batch: 321/469 -- Crit Loss: -3.0100 -- Gen Loss: -25.6547\n",
      "Epoch: 24/50 -- Step: 286 -- Batch: 361/469 -- Crit Loss: -1.5464 -- Gen Loss: -20.5736\n",
      "Epoch: 24/50 -- Step: 287 -- Batch: 401/469 -- Crit Loss: -1.6059 -- Gen Loss: -30.2023\n",
      "Epoch: 24/50 -- Step: 288 -- Batch: 441/469 -- Crit Loss: -1.8609 -- Gen Loss: -32.7505\n",
      "Epoch: 25/50 -- Step: 289 -- Batch: 1/469 -- Crit Loss: -2.7221 -- Gen Loss: -26.9136\n",
      "Epoch: 25/50 -- Step: 290 -- Batch: 41/469 -- Crit Loss: -3.0862 -- Gen Loss: -12.3118\n",
      "Epoch: 25/50 -- Step: 291 -- Batch: 81/469 -- Crit Loss: -3.2535 -- Gen Loss: -27.2714\n",
      "Epoch: 25/50 -- Step: 292 -- Batch: 121/469 -- Crit Loss: -2.1852 -- Gen Loss: -23.6515\n",
      "Epoch: 25/50 -- Step: 293 -- Batch: 161/469 -- Crit Loss: -3.0576 -- Gen Loss: -32.2390\n",
      "Epoch: 25/50 -- Step: 294 -- Batch: 201/469 -- Crit Loss: -2.6522 -- Gen Loss: -29.3416\n",
      "Epoch: 25/50 -- Step: 295 -- Batch: 241/469 -- Crit Loss: -2.1286 -- Gen Loss: -25.5411\n",
      "Epoch: 25/50 -- Step: 296 -- Batch: 281/469 -- Crit Loss: -2.2761 -- Gen Loss: -23.7874\n",
      "Epoch: 25/50 -- Step: 297 -- Batch: 321/469 -- Crit Loss: -2.2630 -- Gen Loss: -28.2192\n",
      "Epoch: 25/50 -- Step: 298 -- Batch: 361/469 -- Crit Loss: -2.0383 -- Gen Loss: -31.9115\n",
      "Epoch: 25/50 -- Step: 299 -- Batch: 401/469 -- Crit Loss: -2.9515 -- Gen Loss: -32.7694\n",
      "Epoch: 25/50 -- Step: 300 -- Batch: 441/469 -- Crit Loss: -2.3953 -- Gen Loss: -24.7533\n",
      "Epoch: 26/50 -- Step: 301 -- Batch: 1/469 -- Crit Loss: -2.4706 -- Gen Loss: -32.9013\n",
      "Epoch: 26/50 -- Step: 302 -- Batch: 41/469 -- Crit Loss: -2.8076 -- Gen Loss: -35.8138\n",
      "Epoch: 26/50 -- Step: 303 -- Batch: 81/469 -- Crit Loss: -3.1649 -- Gen Loss: -33.0419\n",
      "Epoch: 26/50 -- Step: 304 -- Batch: 121/469 -- Crit Loss: -2.8252 -- Gen Loss: -35.7774\n",
      "Epoch: 26/50 -- Step: 305 -- Batch: 161/469 -- Crit Loss: -3.2489 -- Gen Loss: -34.9359\n",
      "Epoch: 26/50 -- Step: 306 -- Batch: 201/469 -- Crit Loss: -2.6293 -- Gen Loss: -40.3529\n",
      "Epoch: 26/50 -- Step: 307 -- Batch: 241/469 -- Crit Loss: -2.8196 -- Gen Loss: -41.8021\n",
      "Epoch: 26/50 -- Step: 308 -- Batch: 281/469 -- Crit Loss: -1.9979 -- Gen Loss: -43.5215\n",
      "Epoch: 26/50 -- Step: 309 -- Batch: 321/469 -- Crit Loss: -2.3195 -- Gen Loss: -39.8747\n",
      "Epoch: 26/50 -- Step: 310 -- Batch: 361/469 -- Crit Loss: -2.2565 -- Gen Loss: -35.8783\n",
      "Epoch: 26/50 -- Step: 311 -- Batch: 401/469 -- Crit Loss: -3.7207 -- Gen Loss: -45.3914\n",
      "Epoch: 26/50 -- Step: 312 -- Batch: 441/469 -- Crit Loss: -3.2427 -- Gen Loss: -41.6270\n",
      "Epoch: 27/50 -- Step: 313 -- Batch: 1/469 -- Crit Loss: -2.6952 -- Gen Loss: -44.5567\n",
      "Epoch: 27/50 -- Step: 314 -- Batch: 41/469 -- Crit Loss: -1.9469 -- Gen Loss: -49.1452\n",
      "Epoch: 27/50 -- Step: 315 -- Batch: 81/469 -- Crit Loss: -2.3877 -- Gen Loss: -42.5957\n",
      "Epoch: 27/50 -- Step: 316 -- Batch: 121/469 -- Crit Loss: -2.5319 -- Gen Loss: -43.6722\n",
      "Epoch: 27/50 -- Step: 317 -- Batch: 161/469 -- Crit Loss: -3.7047 -- Gen Loss: -35.0808\n",
      "Epoch: 27/50 -- Step: 318 -- Batch: 201/469 -- Crit Loss: -2.4540 -- Gen Loss: -37.5248\n",
      "Epoch: 27/50 -- Step: 319 -- Batch: 241/469 -- Crit Loss: -2.7338 -- Gen Loss: -49.4652\n",
      "Epoch: 27/50 -- Step: 320 -- Batch: 281/469 -- Crit Loss: -2.8595 -- Gen Loss: -50.0089\n",
      "Epoch: 27/50 -- Step: 321 -- Batch: 321/469 -- Crit Loss: -2.2707 -- Gen Loss: -40.1191\n",
      "Epoch: 27/50 -- Step: 322 -- Batch: 361/469 -- Crit Loss: -3.1297 -- Gen Loss: -44.8090\n",
      "Epoch: 27/50 -- Step: 323 -- Batch: 401/469 -- Crit Loss: -2.3654 -- Gen Loss: -37.3251\n",
      "Epoch: 27/50 -- Step: 324 -- Batch: 441/469 -- Crit Loss: -2.9829 -- Gen Loss: -45.0090\n",
      "Epoch: 28/50 -- Step: 325 -- Batch: 1/469 -- Crit Loss: -2.1015 -- Gen Loss: -37.7166\n",
      "Epoch: 28/50 -- Step: 326 -- Batch: 41/469 -- Crit Loss: -2.6960 -- Gen Loss: -47.6698\n",
      "Epoch: 28/50 -- Step: 327 -- Batch: 81/469 -- Crit Loss: -2.4909 -- Gen Loss: -47.0183\n",
      "Epoch: 28/50 -- Step: 328 -- Batch: 121/469 -- Crit Loss: -2.6119 -- Gen Loss: -49.5806\n",
      "Epoch: 28/50 -- Step: 329 -- Batch: 161/469 -- Crit Loss: -2.4218 -- Gen Loss: -42.5058\n",
      "Epoch: 28/50 -- Step: 330 -- Batch: 201/469 -- Crit Loss: -2.0487 -- Gen Loss: -44.8328\n",
      "Epoch: 28/50 -- Step: 331 -- Batch: 241/469 -- Crit Loss: -3.9534 -- Gen Loss: -50.9589\n",
      "Epoch: 28/50 -- Step: 332 -- Batch: 281/469 -- Crit Loss: -2.3447 -- Gen Loss: -53.4619\n",
      "Epoch: 28/50 -- Step: 333 -- Batch: 321/469 -- Crit Loss: -2.3330 -- Gen Loss: -59.8753\n",
      "Epoch: 28/50 -- Step: 334 -- Batch: 361/469 -- Crit Loss: -1.6216 -- Gen Loss: -50.7699\n",
      "Epoch: 28/50 -- Step: 335 -- Batch: 401/469 -- Crit Loss: -2.5390 -- Gen Loss: -47.6249\n",
      "Epoch: 28/50 -- Step: 336 -- Batch: 441/469 -- Crit Loss: -1.7991 -- Gen Loss: -59.9526\n",
      "Epoch: 29/50 -- Step: 337 -- Batch: 1/469 -- Crit Loss: -2.0778 -- Gen Loss: -53.4776\n",
      "Epoch: 29/50 -- Step: 338 -- Batch: 41/469 -- Crit Loss: -1.6901 -- Gen Loss: -48.1630\n",
      "Epoch: 29/50 -- Step: 339 -- Batch: 81/469 -- Crit Loss: -2.6114 -- Gen Loss: -49.3263\n",
      "Epoch: 29/50 -- Step: 340 -- Batch: 121/469 -- Crit Loss: -3.4374 -- Gen Loss: -58.5394\n",
      "Epoch: 29/50 -- Step: 341 -- Batch: 161/469 -- Crit Loss: -1.4775 -- Gen Loss: -62.3162\n",
      "Epoch: 29/50 -- Step: 342 -- Batch: 201/469 -- Crit Loss: -2.5073 -- Gen Loss: -59.3956\n",
      "Epoch: 29/50 -- Step: 343 -- Batch: 241/469 -- Crit Loss: -1.3211 -- Gen Loss: -50.5364\n",
      "Epoch: 29/50 -- Step: 344 -- Batch: 281/469 -- Crit Loss: -2.3007 -- Gen Loss: -60.3019\n",
      "Epoch: 29/50 -- Step: 345 -- Batch: 321/469 -- Crit Loss: -2.4945 -- Gen Loss: -57.0373\n",
      "Epoch: 29/50 -- Step: 346 -- Batch: 361/469 -- Crit Loss: -1.1558 -- Gen Loss: -55.7447\n",
      "Epoch: 29/50 -- Step: 347 -- Batch: 401/469 -- Crit Loss: -2.3057 -- Gen Loss: -54.1689\n",
      "Epoch: 29/50 -- Step: 348 -- Batch: 441/469 -- Crit Loss: -1.3020 -- Gen Loss: -48.6216\n",
      "Epoch: 30/50 -- Step: 349 -- Batch: 1/469 -- Crit Loss: -1.4878 -- Gen Loss: -52.4727\n",
      "Epoch: 30/50 -- Step: 350 -- Batch: 41/469 -- Crit Loss: -2.9979 -- Gen Loss: -53.7926\n",
      "Epoch: 30/50 -- Step: 351 -- Batch: 81/469 -- Crit Loss: -2.0910 -- Gen Loss: -61.7542\n",
      "Epoch: 30/50 -- Step: 352 -- Batch: 121/469 -- Crit Loss: -2.1298 -- Gen Loss: -56.1414\n",
      "Epoch: 30/50 -- Step: 353 -- Batch: 161/469 -- Crit Loss: -0.8495 -- Gen Loss: -54.2369\n",
      "Epoch: 30/50 -- Step: 354 -- Batch: 201/469 -- Crit Loss: -2.1612 -- Gen Loss: -55.4856\n",
      "Epoch: 30/50 -- Step: 355 -- Batch: 241/469 -- Crit Loss: -1.9607 -- Gen Loss: -65.1488\n",
      "Epoch: 30/50 -- Step: 356 -- Batch: 281/469 -- Crit Loss: -1.8004 -- Gen Loss: -73.8243\n",
      "Epoch: 30/50 -- Step: 357 -- Batch: 321/469 -- Crit Loss: -2.0922 -- Gen Loss: -66.9975\n"
     ]
    }
   ],
   "source": [
    "crit.train(), gen.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real, labels) in enumerate(images_loader):\n",
    "        real = real.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        for _ in range(CRIT_ITER):\n",
    "            noise = torch.randn((real.shape[0], Z_DIM, 1, 1)).to(device)\n",
    "            fake = gen(noise, labels)\n",
    "\n",
    "            crit_real = crit(real, labels).view(-1)\n",
    "            crit_fake = crit(fake, labels).view(-1)\n",
    "            gp = find_gradient_penalty(crit, labels, real, fake, device)\n",
    "            loss_crit = (\n",
    "                -(torch.mean(crit_real) - torch.mean(crit_fake)) + LAMBDA_GP*gp\n",
    "                )\n",
    "            crit.zero_grad()\n",
    "            loss_crit.backward(retain_graph=True)\n",
    "            optim_crit.step()\n",
    "\n",
    "        crit_fake = crit(fake, labels).view(-1)\n",
    "        loss_gen = -torch.mean(crit_fake)\n",
    "\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        optim_gen.step()\n",
    "\n",
    "        if batch_idx % 40 == 0:\n",
    "            print(\n",
    "                f'Epoch: {epoch+1}/{NUM_EPOCHS} -- Step: {steps} -- Batch: {batch_idx+1}/{len(images_loader)} -- Crit Loss: {loss_crit:.4f} -- Gen Loss: {loss_gen:.4f}'\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                fixed_noise = torch.randn((GRID_SHOW, Z_DIM, 1, 1)).to(device)\n",
    "                fake = gen(fixed_noise, fixed_labels)\n",
    "\n",
    "                fake_images = torchvision.utils.make_grid(fake, nrow=5, normalize=True)\n",
    "                save_image(fake_images, f'wgangp_results/{steps}.png')\n",
    "                real_images = torchvision.utils.make_grid(real[:GRID_SHOW], nrow=5, normalize=True)\n",
    "                writer_fake.add_scalar('Gen Loss', loss_gen, global_step=steps)\n",
    "                writer_fake.add_image(\n",
    "                    'Fake', fake_images, global_step=steps\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    'Real', real_images, global_step=steps\n",
    "                )\n",
    "                steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
