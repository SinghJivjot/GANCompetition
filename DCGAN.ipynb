{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "root_path = '/mnt/c/Users/121js/OneDrive/Desktop/TorchImages/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Disc and Gen classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes, img_size, img_channels=3, features=128): \n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self._make_conv2d_block(img_channels+1, features, use_bn=False), \n",
    "\n",
    "            self._make_conv2d_block(features, features*2), \n",
    "            self._make_conv2d_block(features*2, features*4), \n",
    "            self._make_conv2d_block(features*4, features*8), \n",
    "\n",
    "            self._make_conv2d_block(features*8, 1, 4, 2, 0, use_bn=False, use_act=False), \n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, img_size*img_size)\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def _make_conv2d_block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, use_bn=True, use_act=True, leak=0.2):\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "        ]\n",
    "        if use_bn:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        if use_act:\n",
    "            layers.append(nn.LeakyReLU(leak))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        x = torch.cat([x, self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)], axis=1)\n",
    "        return self.disc(x)\n",
    "    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_classes, embed_size, z_dim=100, img_channels=3, features=1024):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._make_convT2d_block(z_dim+embed_size, features*8, 4, 1, 0), \n",
    "\n",
    "            self._make_convT2d_block(features*8, features*4), \n",
    "            self._make_convT2d_block(features*4, features*2), \n",
    "            self._make_convT2d_block(features*2, features), \n",
    "\n",
    "            self._make_convT2d_block(features, img_channels, use_bn=False, use_act=False), \n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, embed_size)\n",
    "\n",
    "    def _make_convT2d_block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, use_bn=True, use_act=True):\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "        ]\n",
    "        if use_bn:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        if use_act:\n",
    "            layers.append(nn.ReLU())\n",
    "        return nn.Sequential(*layers)        \n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        x = torch.cat([x, self.embed(labels).unsqueeze(2).unsqueeze(3)], axis=1)\n",
    "        return self.gen(x)\n",
    "    \n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(\n",
    "                tensor=m.weight.data,\n",
    "                mean=0.0,\n",
    "                std=0.02,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE, NUM_CHANNELS, H, W = 8, 3, 64, 64\n",
    "# Z_DIM = 100\n",
    "# FEATURES = 64\n",
    "# NUM_CLASSES = 10\n",
    "# LABELS = torch.arange(8)\n",
    "\n",
    "# img_test = torch.randn((BATCH_SIZE, NUM_CHANNELS, H, W))\n",
    "# disc = Discriminator(NUM_CLASSES, H, NUM_CHANNELS, FEATURES)\n",
    "# assert disc(img_test, LABELS).shape == (BATCH_SIZE, 1, 1, 1), 'Disc Test Failed'\n",
    "\n",
    "# noise_test = torch.randn((BATCH_SIZE, Z_DIM, 1, 1))\n",
    "# gen = Generator(NUM_CLASSES, H, Z_DIM, NUM_CHANNELS, FEATURES)\n",
    "# assert gen(noise_test, LABELS).shape == (BATCH_SIZE, NUM_CHANNELS, H, W), 'Gen Test Failed'\n",
    "\n",
    "# print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 128 \n",
    "NUM_CHANNELS = 1\n",
    "IMG_SIZE = 64\n",
    "NUM_CLASSES = 10\n",
    "GEN_EMBEDDING = 100\n",
    "Z_DIM = 100\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 128\n",
    "LR = 1e-4\n",
    "NUM_EPOCHS = 50\n",
    "DISC_ITER = 1\n",
    "GRID_SHOW = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.Grayscale(NUM_CHANNELS),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(NUM_CHANNELS)], [0.5 for _ in range(NUM_CHANNELS)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# images = datasets.ImageFolder(root=root_path + 'dogs/images', transform=transformations)\n",
    "images = datasets.MNIST(root='mnist', train=True, transform=transformations, download=True)\n",
    "images_loader = DataLoader(dataset=images, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "disc = Discriminator(NUM_CLASSES, IMG_SIZE, NUM_CHANNELS, FEATURES_DISC).to(DEVICE)\n",
    "gen = Generator(NUM_CLASSES, GEN_EMBEDDING, Z_DIM, NUM_CHANNELS, FEATURES_GEN).to(DEVICE)\n",
    "initialize_weights(disc), initialize_weights(gen)\n",
    "\n",
    "fixed_labels = torch.arange(10).to(DEVICE)\n",
    "optim_disc = optim.Adam(disc.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "optim_gen = optim.Adam(gen.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "writer_fake = SummaryWriter('logs/fake')\n",
    "writer_real = SummaryWriter('logs/real')\n",
    "steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Size: 1.429e+07, Discriminator Size: 2.806e+06\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Generator Size: {model_size(gen):.3e}, Discriminator Size: {model_size(disc):.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 -- Step: 1 -- Batch: 1/469 -- Disc Loss: 1.3881 -- Gen Loss: 0.7137\n",
      "Epoch: 1/50 -- Step: 2 -- Batch: 41/469 -- Disc Loss: 0.3444 -- Gen Loss: 2.0648\n",
      "Epoch: 1/50 -- Step: 3 -- Batch: 81/469 -- Disc Loss: 0.1026 -- Gen Loss: 3.2163\n",
      "Epoch: 1/50 -- Step: 4 -- Batch: 121/469 -- Disc Loss: 0.0511 -- Gen Loss: 3.8288\n",
      "Epoch: 1/50 -- Step: 5 -- Batch: 161/469 -- Disc Loss: 0.0301 -- Gen Loss: 4.3112\n",
      "Epoch: 1/50 -- Step: 6 -- Batch: 201/469 -- Disc Loss: 0.0203 -- Gen Loss: 4.6997\n",
      "Epoch: 1/50 -- Step: 7 -- Batch: 241/469 -- Disc Loss: 0.1415 -- Gen Loss: 3.1004\n",
      "Epoch: 1/50 -- Step: 8 -- Batch: 281/469 -- Disc Loss: 0.0392 -- Gen Loss: 4.0925\n",
      "Epoch: 1/50 -- Step: 9 -- Batch: 321/469 -- Disc Loss: 2.8101 -- Gen Loss: 0.1479\n",
      "Epoch: 1/50 -- Step: 10 -- Batch: 361/469 -- Disc Loss: 0.5958 -- Gen Loss: 1.8808\n",
      "Epoch: 1/50 -- Step: 11 -- Batch: 401/469 -- Disc Loss: 0.6492 -- Gen Loss: 1.9418\n",
      "Epoch: 1/50 -- Step: 12 -- Batch: 441/469 -- Disc Loss: 0.6691 -- Gen Loss: 2.4900\n",
      "Epoch: 2/50 -- Step: 13 -- Batch: 1/469 -- Disc Loss: 0.7658 -- Gen Loss: 2.6399\n",
      "Epoch: 2/50 -- Step: 14 -- Batch: 41/469 -- Disc Loss: 0.9721 -- Gen Loss: 2.4691\n",
      "Epoch: 2/50 -- Step: 15 -- Batch: 81/469 -- Disc Loss: 1.4808 -- Gen Loss: 2.0875\n",
      "Epoch: 2/50 -- Step: 16 -- Batch: 121/469 -- Disc Loss: 1.2355 -- Gen Loss: 1.8708\n",
      "Epoch: 2/50 -- Step: 17 -- Batch: 161/469 -- Disc Loss: 1.0684 -- Gen Loss: 1.6247\n",
      "Epoch: 2/50 -- Step: 18 -- Batch: 201/469 -- Disc Loss: 1.2058 -- Gen Loss: 1.9021\n",
      "Epoch: 2/50 -- Step: 19 -- Batch: 241/469 -- Disc Loss: 1.1932 -- Gen Loss: 0.8612\n",
      "Epoch: 2/50 -- Step: 20 -- Batch: 281/469 -- Disc Loss: 1.4043 -- Gen Loss: 0.7207\n",
      "Epoch: 2/50 -- Step: 21 -- Batch: 321/469 -- Disc Loss: 1.1397 -- Gen Loss: 1.4566\n",
      "Epoch: 2/50 -- Step: 22 -- Batch: 361/469 -- Disc Loss: 1.2102 -- Gen Loss: 1.3481\n",
      "Epoch: 2/50 -- Step: 23 -- Batch: 401/469 -- Disc Loss: 1.1856 -- Gen Loss: 1.0968\n",
      "Epoch: 2/50 -- Step: 24 -- Batch: 441/469 -- Disc Loss: 1.1456 -- Gen Loss: 0.7103\n",
      "Epoch: 3/50 -- Step: 25 -- Batch: 1/469 -- Disc Loss: 1.2336 -- Gen Loss: 0.9700\n",
      "Epoch: 3/50 -- Step: 26 -- Batch: 41/469 -- Disc Loss: 1.0631 -- Gen Loss: 1.1743\n",
      "Epoch: 3/50 -- Step: 27 -- Batch: 81/469 -- Disc Loss: 1.1588 -- Gen Loss: 0.8483\n",
      "Epoch: 3/50 -- Step: 28 -- Batch: 121/469 -- Disc Loss: 1.1794 -- Gen Loss: 1.2823\n",
      "Epoch: 3/50 -- Step: 29 -- Batch: 161/469 -- Disc Loss: 1.1823 -- Gen Loss: 1.1041\n",
      "Epoch: 3/50 -- Step: 30 -- Batch: 201/469 -- Disc Loss: 1.1462 -- Gen Loss: 1.3570\n",
      "Epoch: 3/50 -- Step: 31 -- Batch: 241/469 -- Disc Loss: 1.2922 -- Gen Loss: 1.1163\n",
      "Epoch: 3/50 -- Step: 32 -- Batch: 281/469 -- Disc Loss: 1.4158 -- Gen Loss: 0.7079\n",
      "Epoch: 3/50 -- Step: 33 -- Batch: 321/469 -- Disc Loss: 1.2181 -- Gen Loss: 1.0946\n",
      "Epoch: 3/50 -- Step: 34 -- Batch: 361/469 -- Disc Loss: 1.1654 -- Gen Loss: 1.2366\n",
      "Epoch: 3/50 -- Step: 35 -- Batch: 401/469 -- Disc Loss: 1.2413 -- Gen Loss: 0.7910\n",
      "Epoch: 3/50 -- Step: 36 -- Batch: 441/469 -- Disc Loss: 1.1753 -- Gen Loss: 1.2905\n",
      "Epoch: 4/50 -- Step: 37 -- Batch: 1/469 -- Disc Loss: 1.0152 -- Gen Loss: 1.3049\n",
      "Epoch: 4/50 -- Step: 38 -- Batch: 41/469 -- Disc Loss: 1.1841 -- Gen Loss: 0.7602\n",
      "Epoch: 4/50 -- Step: 39 -- Batch: 81/469 -- Disc Loss: 1.1533 -- Gen Loss: 0.9903\n",
      "Epoch: 4/50 -- Step: 40 -- Batch: 121/469 -- Disc Loss: 1.1927 -- Gen Loss: 0.7304\n",
      "Epoch: 4/50 -- Step: 41 -- Batch: 161/469 -- Disc Loss: 1.0235 -- Gen Loss: 1.7209\n",
      "Epoch: 4/50 -- Step: 42 -- Batch: 201/469 -- Disc Loss: 1.1751 -- Gen Loss: 1.1201\n",
      "Epoch: 4/50 -- Step: 43 -- Batch: 241/469 -- Disc Loss: 1.2123 -- Gen Loss: 0.6557\n",
      "Epoch: 4/50 -- Step: 44 -- Batch: 281/469 -- Disc Loss: 1.1957 -- Gen Loss: 1.1626\n",
      "Epoch: 4/50 -- Step: 45 -- Batch: 321/469 -- Disc Loss: 1.0796 -- Gen Loss: 0.9417\n",
      "Epoch: 4/50 -- Step: 46 -- Batch: 361/469 -- Disc Loss: 1.2757 -- Gen Loss: 1.4057\n",
      "Epoch: 4/50 -- Step: 47 -- Batch: 401/469 -- Disc Loss: 1.1155 -- Gen Loss: 1.2967\n",
      "Epoch: 4/50 -- Step: 48 -- Batch: 441/469 -- Disc Loss: 1.1868 -- Gen Loss: 1.3879\n",
      "Epoch: 5/50 -- Step: 49 -- Batch: 1/469 -- Disc Loss: 1.2802 -- Gen Loss: 1.6319\n",
      "Epoch: 5/50 -- Step: 50 -- Batch: 41/469 -- Disc Loss: 1.2137 -- Gen Loss: 1.1567\n",
      "Epoch: 5/50 -- Step: 51 -- Batch: 81/469 -- Disc Loss: 1.2115 -- Gen Loss: 0.5843\n",
      "Epoch: 5/50 -- Step: 52 -- Batch: 121/469 -- Disc Loss: 1.3294 -- Gen Loss: 1.4765\n",
      "Epoch: 5/50 -- Step: 53 -- Batch: 161/469 -- Disc Loss: 1.1792 -- Gen Loss: 1.7534\n",
      "Epoch: 5/50 -- Step: 54 -- Batch: 201/469 -- Disc Loss: 1.2362 -- Gen Loss: 1.4948\n",
      "Epoch: 5/50 -- Step: 55 -- Batch: 241/469 -- Disc Loss: 1.1272 -- Gen Loss: 1.0172\n",
      "Epoch: 5/50 -- Step: 56 -- Batch: 281/469 -- Disc Loss: 1.0787 -- Gen Loss: 1.5294\n",
      "Epoch: 5/50 -- Step: 57 -- Batch: 321/469 -- Disc Loss: 1.2328 -- Gen Loss: 1.0273\n",
      "Epoch: 5/50 -- Step: 58 -- Batch: 361/469 -- Disc Loss: 1.1862 -- Gen Loss: 1.5020\n",
      "Epoch: 5/50 -- Step: 59 -- Batch: 401/469 -- Disc Loss: 0.9983 -- Gen Loss: 1.1213\n",
      "Epoch: 5/50 -- Step: 60 -- Batch: 441/469 -- Disc Loss: 1.1071 -- Gen Loss: 1.1129\n",
      "Epoch: 6/50 -- Step: 61 -- Batch: 1/469 -- Disc Loss: 1.2721 -- Gen Loss: 1.3435\n",
      "Epoch: 6/50 -- Step: 62 -- Batch: 41/469 -- Disc Loss: 0.9660 -- Gen Loss: 1.6172\n",
      "Epoch: 6/50 -- Step: 63 -- Batch: 81/469 -- Disc Loss: 1.1518 -- Gen Loss: 1.6555\n",
      "Epoch: 6/50 -- Step: 64 -- Batch: 121/469 -- Disc Loss: 1.1012 -- Gen Loss: 0.8167\n",
      "Epoch: 6/50 -- Step: 65 -- Batch: 161/469 -- Disc Loss: 1.0587 -- Gen Loss: 1.6975\n",
      "Epoch: 6/50 -- Step: 66 -- Batch: 201/469 -- Disc Loss: 0.9804 -- Gen Loss: 1.1622\n",
      "Epoch: 6/50 -- Step: 67 -- Batch: 241/469 -- Disc Loss: 1.0530 -- Gen Loss: 1.3247\n",
      "Epoch: 6/50 -- Step: 68 -- Batch: 281/469 -- Disc Loss: 1.0054 -- Gen Loss: 1.1046\n",
      "Epoch: 6/50 -- Step: 69 -- Batch: 321/469 -- Disc Loss: 0.9293 -- Gen Loss: 2.2940\n",
      "Epoch: 6/50 -- Step: 70 -- Batch: 361/469 -- Disc Loss: 0.9655 -- Gen Loss: 1.8891\n",
      "Epoch: 6/50 -- Step: 71 -- Batch: 401/469 -- Disc Loss: 1.0026 -- Gen Loss: 1.2678\n",
      "Epoch: 6/50 -- Step: 72 -- Batch: 441/469 -- Disc Loss: 0.8882 -- Gen Loss: 1.3773\n",
      "Epoch: 7/50 -- Step: 73 -- Batch: 1/469 -- Disc Loss: 1.0386 -- Gen Loss: 1.8681\n",
      "Epoch: 7/50 -- Step: 74 -- Batch: 41/469 -- Disc Loss: 0.9373 -- Gen Loss: 1.1656\n",
      "Epoch: 7/50 -- Step: 75 -- Batch: 81/469 -- Disc Loss: 1.0104 -- Gen Loss: 1.5070\n",
      "Epoch: 7/50 -- Step: 76 -- Batch: 121/469 -- Disc Loss: 1.1114 -- Gen Loss: 1.4776\n",
      "Epoch: 7/50 -- Step: 77 -- Batch: 161/469 -- Disc Loss: 1.1962 -- Gen Loss: 1.5049\n",
      "Epoch: 7/50 -- Step: 78 -- Batch: 201/469 -- Disc Loss: 0.8565 -- Gen Loss: 1.5019\n",
      "Epoch: 7/50 -- Step: 79 -- Batch: 241/469 -- Disc Loss: 1.2198 -- Gen Loss: 2.6183\n",
      "Epoch: 7/50 -- Step: 80 -- Batch: 281/469 -- Disc Loss: 0.9636 -- Gen Loss: 2.6707\n",
      "Epoch: 7/50 -- Step: 81 -- Batch: 321/469 -- Disc Loss: 0.8486 -- Gen Loss: 1.3201\n",
      "Epoch: 7/50 -- Step: 82 -- Batch: 361/469 -- Disc Loss: 0.9555 -- Gen Loss: 2.0733\n",
      "Epoch: 7/50 -- Step: 83 -- Batch: 401/469 -- Disc Loss: 1.0595 -- Gen Loss: 1.4598\n",
      "Epoch: 7/50 -- Step: 84 -- Batch: 441/469 -- Disc Loss: 1.1519 -- Gen Loss: 1.0590\n",
      "Epoch: 8/50 -- Step: 85 -- Batch: 1/469 -- Disc Loss: 1.0042 -- Gen Loss: 2.9059\n",
      "Epoch: 8/50 -- Step: 86 -- Batch: 41/469 -- Disc Loss: 1.0129 -- Gen Loss: 1.4483\n",
      "Epoch: 8/50 -- Step: 87 -- Batch: 81/469 -- Disc Loss: 0.9717 -- Gen Loss: 1.1619\n",
      "Epoch: 8/50 -- Step: 88 -- Batch: 121/469 -- Disc Loss: 0.8904 -- Gen Loss: 1.9191\n",
      "Epoch: 8/50 -- Step: 89 -- Batch: 161/469 -- Disc Loss: 1.0130 -- Gen Loss: 1.1763\n",
      "Epoch: 8/50 -- Step: 90 -- Batch: 201/469 -- Disc Loss: 0.7501 -- Gen Loss: 1.6376\n",
      "Epoch: 8/50 -- Step: 91 -- Batch: 241/469 -- Disc Loss: 0.6517 -- Gen Loss: 1.7390\n",
      "Epoch: 8/50 -- Step: 92 -- Batch: 281/469 -- Disc Loss: 0.7747 -- Gen Loss: 1.5119\n",
      "Epoch: 8/50 -- Step: 93 -- Batch: 321/469 -- Disc Loss: 1.0103 -- Gen Loss: 1.8102\n",
      "Epoch: 8/50 -- Step: 94 -- Batch: 361/469 -- Disc Loss: 0.9405 -- Gen Loss: 1.6354\n",
      "Epoch: 8/50 -- Step: 95 -- Batch: 401/469 -- Disc Loss: 0.8045 -- Gen Loss: 1.5020\n",
      "Epoch: 8/50 -- Step: 96 -- Batch: 441/469 -- Disc Loss: 0.6987 -- Gen Loss: 1.7645\n",
      "Epoch: 9/50 -- Step: 97 -- Batch: 1/469 -- Disc Loss: 0.8938 -- Gen Loss: 1.4453\n",
      "Epoch: 9/50 -- Step: 98 -- Batch: 41/469 -- Disc Loss: 0.8180 -- Gen Loss: 1.2096\n",
      "Epoch: 9/50 -- Step: 99 -- Batch: 81/469 -- Disc Loss: 0.9828 -- Gen Loss: 1.3955\n",
      "Epoch: 9/50 -- Step: 100 -- Batch: 121/469 -- Disc Loss: 0.9029 -- Gen Loss: 1.7848\n",
      "Epoch: 9/50 -- Step: 101 -- Batch: 161/469 -- Disc Loss: 1.0402 -- Gen Loss: 2.2454\n",
      "Epoch: 9/50 -- Step: 102 -- Batch: 201/469 -- Disc Loss: 0.9085 -- Gen Loss: 1.4629\n",
      "Epoch: 9/50 -- Step: 103 -- Batch: 241/469 -- Disc Loss: 0.7146 -- Gen Loss: 1.6466\n",
      "Epoch: 9/50 -- Step: 104 -- Batch: 281/469 -- Disc Loss: 0.9410 -- Gen Loss: 1.9113\n",
      "Epoch: 9/50 -- Step: 105 -- Batch: 321/469 -- Disc Loss: 0.8935 -- Gen Loss: 1.3963\n",
      "Epoch: 9/50 -- Step: 106 -- Batch: 361/469 -- Disc Loss: 0.8416 -- Gen Loss: 1.7538\n",
      "Epoch: 9/50 -- Step: 107 -- Batch: 401/469 -- Disc Loss: 0.7525 -- Gen Loss: 1.5590\n",
      "Epoch: 9/50 -- Step: 108 -- Batch: 441/469 -- Disc Loss: 0.6263 -- Gen Loss: 2.1875\n",
      "Epoch: 10/50 -- Step: 109 -- Batch: 1/469 -- Disc Loss: 0.8020 -- Gen Loss: 2.7446\n",
      "Epoch: 10/50 -- Step: 110 -- Batch: 41/469 -- Disc Loss: 0.7165 -- Gen Loss: 1.4596\n",
      "Epoch: 10/50 -- Step: 111 -- Batch: 81/469 -- Disc Loss: 0.9703 -- Gen Loss: 1.7356\n",
      "Epoch: 10/50 -- Step: 112 -- Batch: 121/469 -- Disc Loss: 0.7923 -- Gen Loss: 1.8482\n",
      "Epoch: 10/50 -- Step: 113 -- Batch: 161/469 -- Disc Loss: 0.7180 -- Gen Loss: 1.5996\n",
      "Epoch: 10/50 -- Step: 114 -- Batch: 201/469 -- Disc Loss: 0.5581 -- Gen Loss: 2.1178\n",
      "Epoch: 10/50 -- Step: 115 -- Batch: 241/469 -- Disc Loss: 0.8674 -- Gen Loss: 1.9766\n",
      "Epoch: 10/50 -- Step: 116 -- Batch: 281/469 -- Disc Loss: 0.8741 -- Gen Loss: 3.2946\n",
      "Epoch: 10/50 -- Step: 117 -- Batch: 321/469 -- Disc Loss: 0.4193 -- Gen Loss: 2.5271\n",
      "Epoch: 10/50 -- Step: 118 -- Batch: 361/469 -- Disc Loss: 0.7590 -- Gen Loss: 1.7840\n",
      "Epoch: 10/50 -- Step: 119 -- Batch: 401/469 -- Disc Loss: 0.7470 -- Gen Loss: 1.6972\n",
      "Epoch: 10/50 -- Step: 120 -- Batch: 441/469 -- Disc Loss: 1.0830 -- Gen Loss: 1.5163\n",
      "Epoch: 11/50 -- Step: 121 -- Batch: 1/469 -- Disc Loss: 1.2135 -- Gen Loss: 0.9634\n",
      "Epoch: 11/50 -- Step: 122 -- Batch: 41/469 -- Disc Loss: 0.6737 -- Gen Loss: 1.9725\n",
      "Epoch: 11/50 -- Step: 123 -- Batch: 81/469 -- Disc Loss: 0.8140 -- Gen Loss: 1.4523\n",
      "Epoch: 11/50 -- Step: 124 -- Batch: 121/469 -- Disc Loss: 0.8250 -- Gen Loss: 1.3039\n",
      "Epoch: 11/50 -- Step: 125 -- Batch: 161/469 -- Disc Loss: 0.8469 -- Gen Loss: 2.3343\n",
      "Epoch: 11/50 -- Step: 126 -- Batch: 201/469 -- Disc Loss: 0.5442 -- Gen Loss: 2.0271\n",
      "Epoch: 11/50 -- Step: 127 -- Batch: 241/469 -- Disc Loss: 0.5991 -- Gen Loss: 2.0706\n",
      "Epoch: 11/50 -- Step: 128 -- Batch: 281/469 -- Disc Loss: 0.6062 -- Gen Loss: 3.1150\n",
      "Epoch: 11/50 -- Step: 129 -- Batch: 321/469 -- Disc Loss: 0.7258 -- Gen Loss: 2.8390\n",
      "Epoch: 11/50 -- Step: 130 -- Batch: 361/469 -- Disc Loss: 0.8404 -- Gen Loss: 2.3071\n",
      "Epoch: 11/50 -- Step: 131 -- Batch: 401/469 -- Disc Loss: 0.9253 -- Gen Loss: 2.3386\n",
      "Epoch: 11/50 -- Step: 132 -- Batch: 441/469 -- Disc Loss: 0.6436 -- Gen Loss: 2.2305\n",
      "Epoch: 12/50 -- Step: 133 -- Batch: 1/469 -- Disc Loss: 0.8538 -- Gen Loss: 2.5759\n",
      "Epoch: 12/50 -- Step: 134 -- Batch: 41/469 -- Disc Loss: 0.7504 -- Gen Loss: 1.4333\n",
      "Epoch: 12/50 -- Step: 135 -- Batch: 81/469 -- Disc Loss: 0.8069 -- Gen Loss: 2.6290\n",
      "Epoch: 12/50 -- Step: 136 -- Batch: 121/469 -- Disc Loss: 0.7011 -- Gen Loss: 2.1450\n",
      "Epoch: 12/50 -- Step: 137 -- Batch: 161/469 -- Disc Loss: 1.0144 -- Gen Loss: 4.0047\n",
      "Epoch: 12/50 -- Step: 138 -- Batch: 201/469 -- Disc Loss: 0.5443 -- Gen Loss: 2.2651\n",
      "Epoch: 12/50 -- Step: 139 -- Batch: 241/469 -- Disc Loss: 0.8038 -- Gen Loss: 1.2432\n",
      "Epoch: 12/50 -- Step: 140 -- Batch: 281/469 -- Disc Loss: 0.6126 -- Gen Loss: 3.0454\n",
      "Epoch: 12/50 -- Step: 141 -- Batch: 321/469 -- Disc Loss: 0.4606 -- Gen Loss: 2.6158\n",
      "Epoch: 12/50 -- Step: 142 -- Batch: 361/469 -- Disc Loss: 0.5579 -- Gen Loss: 2.2380\n",
      "Epoch: 12/50 -- Step: 143 -- Batch: 401/469 -- Disc Loss: 0.4112 -- Gen Loss: 2.4076\n",
      "Epoch: 12/50 -- Step: 144 -- Batch: 441/469 -- Disc Loss: 0.7675 -- Gen Loss: 1.5396\n",
      "Epoch: 13/50 -- Step: 145 -- Batch: 1/469 -- Disc Loss: 0.9120 -- Gen Loss: 1.6050\n",
      "Epoch: 13/50 -- Step: 146 -- Batch: 41/469 -- Disc Loss: 0.8646 -- Gen Loss: 1.8175\n",
      "Epoch: 13/50 -- Step: 147 -- Batch: 81/469 -- Disc Loss: 0.5926 -- Gen Loss: 2.9916\n",
      "Epoch: 13/50 -- Step: 148 -- Batch: 121/469 -- Disc Loss: 0.5922 -- Gen Loss: 2.1802\n",
      "Epoch: 13/50 -- Step: 149 -- Batch: 161/469 -- Disc Loss: 1.0392 -- Gen Loss: 1.0848\n",
      "Epoch: 13/50 -- Step: 150 -- Batch: 201/469 -- Disc Loss: 0.4907 -- Gen Loss: 2.1757\n",
      "Epoch: 13/50 -- Step: 151 -- Batch: 241/469 -- Disc Loss: 0.8804 -- Gen Loss: 1.1581\n",
      "Epoch: 13/50 -- Step: 152 -- Batch: 281/469 -- Disc Loss: 0.9644 -- Gen Loss: 4.0255\n",
      "Epoch: 13/50 -- Step: 153 -- Batch: 321/469 -- Disc Loss: 0.7811 -- Gen Loss: 1.5732\n",
      "Epoch: 13/50 -- Step: 154 -- Batch: 361/469 -- Disc Loss: 0.5660 -- Gen Loss: 2.7576\n",
      "Epoch: 13/50 -- Step: 155 -- Batch: 401/469 -- Disc Loss: 0.6530 -- Gen Loss: 2.3280\n",
      "Epoch: 13/50 -- Step: 156 -- Batch: 441/469 -- Disc Loss: 0.6340 -- Gen Loss: 2.0685\n",
      "Epoch: 14/50 -- Step: 157 -- Batch: 1/469 -- Disc Loss: 0.6833 -- Gen Loss: 2.0408\n",
      "Epoch: 14/50 -- Step: 158 -- Batch: 41/469 -- Disc Loss: 0.4720 -- Gen Loss: 2.5456\n",
      "Epoch: 14/50 -- Step: 159 -- Batch: 81/469 -- Disc Loss: 0.5260 -- Gen Loss: 2.2376\n",
      "Epoch: 14/50 -- Step: 160 -- Batch: 121/469 -- Disc Loss: 0.5003 -- Gen Loss: 2.3793\n",
      "Epoch: 14/50 -- Step: 161 -- Batch: 161/469 -- Disc Loss: 0.8781 -- Gen Loss: 1.2774\n",
      "Epoch: 14/50 -- Step: 162 -- Batch: 201/469 -- Disc Loss: 0.5650 -- Gen Loss: 2.1848\n",
      "Epoch: 14/50 -- Step: 163 -- Batch: 241/469 -- Disc Loss: 0.6746 -- Gen Loss: 1.6859\n",
      "Epoch: 14/50 -- Step: 164 -- Batch: 281/469 -- Disc Loss: 0.7924 -- Gen Loss: 3.5102\n",
      "Epoch: 14/50 -- Step: 165 -- Batch: 321/469 -- Disc Loss: 0.4273 -- Gen Loss: 3.1395\n",
      "Epoch: 14/50 -- Step: 166 -- Batch: 361/469 -- Disc Loss: 0.9125 -- Gen Loss: 1.1645\n",
      "Epoch: 14/50 -- Step: 167 -- Batch: 401/469 -- Disc Loss: 0.7244 -- Gen Loss: 0.9928\n",
      "Epoch: 14/50 -- Step: 168 -- Batch: 441/469 -- Disc Loss: 0.5612 -- Gen Loss: 2.9747\n",
      "Epoch: 15/50 -- Step: 169 -- Batch: 1/469 -- Disc Loss: 0.5523 -- Gen Loss: 2.1040\n",
      "Epoch: 15/50 -- Step: 170 -- Batch: 41/469 -- Disc Loss: 0.7390 -- Gen Loss: 1.0951\n",
      "Epoch: 15/50 -- Step: 171 -- Batch: 81/469 -- Disc Loss: 1.7852 -- Gen Loss: 3.8322\n",
      "Epoch: 15/50 -- Step: 172 -- Batch: 121/469 -- Disc Loss: 0.5290 -- Gen Loss: 3.0859\n",
      "Epoch: 15/50 -- Step: 173 -- Batch: 161/469 -- Disc Loss: 0.5091 -- Gen Loss: 2.3036\n",
      "Epoch: 15/50 -- Step: 174 -- Batch: 201/469 -- Disc Loss: 0.6401 -- Gen Loss: 2.2766\n",
      "Epoch: 15/50 -- Step: 175 -- Batch: 241/469 -- Disc Loss: 0.7017 -- Gen Loss: 2.0011\n",
      "Epoch: 15/50 -- Step: 176 -- Batch: 281/469 -- Disc Loss: 0.7011 -- Gen Loss: 3.3580\n",
      "Epoch: 15/50 -- Step: 177 -- Batch: 321/469 -- Disc Loss: 0.6501 -- Gen Loss: 1.3093\n",
      "Epoch: 15/50 -- Step: 178 -- Batch: 361/469 -- Disc Loss: 0.6151 -- Gen Loss: 4.0612\n",
      "Epoch: 15/50 -- Step: 179 -- Batch: 401/469 -- Disc Loss: 0.4355 -- Gen Loss: 2.3293\n",
      "Epoch: 15/50 -- Step: 180 -- Batch: 441/469 -- Disc Loss: 1.5982 -- Gen Loss: 0.7402\n",
      "Epoch: 16/50 -- Step: 181 -- Batch: 1/469 -- Disc Loss: 0.4736 -- Gen Loss: 2.5417\n",
      "Epoch: 16/50 -- Step: 182 -- Batch: 41/469 -- Disc Loss: 0.9791 -- Gen Loss: 1.9283\n",
      "Epoch: 16/50 -- Step: 183 -- Batch: 81/469 -- Disc Loss: 0.3973 -- Gen Loss: 2.6968\n",
      "Epoch: 16/50 -- Step: 184 -- Batch: 121/469 -- Disc Loss: 0.4481 -- Gen Loss: 2.0942\n",
      "Epoch: 16/50 -- Step: 185 -- Batch: 161/469 -- Disc Loss: 0.6773 -- Gen Loss: 2.6023\n",
      "Epoch: 16/50 -- Step: 186 -- Batch: 201/469 -- Disc Loss: 0.4597 -- Gen Loss: 3.0118\n",
      "Epoch: 16/50 -- Step: 187 -- Batch: 241/469 -- Disc Loss: 0.3415 -- Gen Loss: 3.0821\n",
      "Epoch: 16/50 -- Step: 188 -- Batch: 281/469 -- Disc Loss: 0.7633 -- Gen Loss: 1.4297\n",
      "Epoch: 16/50 -- Step: 189 -- Batch: 321/469 -- Disc Loss: 0.7296 -- Gen Loss: 3.5429\n",
      "Epoch: 16/50 -- Step: 190 -- Batch: 361/469 -- Disc Loss: 0.6165 -- Gen Loss: 1.5493\n",
      "Epoch: 16/50 -- Step: 191 -- Batch: 401/469 -- Disc Loss: 0.3470 -- Gen Loss: 2.1749\n",
      "Epoch: 16/50 -- Step: 192 -- Batch: 441/469 -- Disc Loss: 0.5789 -- Gen Loss: 1.9126\n",
      "Epoch: 17/50 -- Step: 193 -- Batch: 1/469 -- Disc Loss: 0.7750 -- Gen Loss: 2.9403\n",
      "Epoch: 17/50 -- Step: 194 -- Batch: 41/469 -- Disc Loss: 0.6177 -- Gen Loss: 3.3826\n",
      "Epoch: 17/50 -- Step: 195 -- Batch: 81/469 -- Disc Loss: 0.7206 -- Gen Loss: 4.0594\n",
      "Epoch: 17/50 -- Step: 196 -- Batch: 121/469 -- Disc Loss: 0.3170 -- Gen Loss: 3.1355\n",
      "Epoch: 17/50 -- Step: 197 -- Batch: 161/469 -- Disc Loss: 0.4175 -- Gen Loss: 2.0365\n",
      "Epoch: 17/50 -- Step: 198 -- Batch: 201/469 -- Disc Loss: 1.1149 -- Gen Loss: 1.7084\n",
      "Epoch: 17/50 -- Step: 199 -- Batch: 241/469 -- Disc Loss: 0.2366 -- Gen Loss: 3.2033\n",
      "Epoch: 17/50 -- Step: 200 -- Batch: 281/469 -- Disc Loss: 0.6044 -- Gen Loss: 4.5663\n",
      "Epoch: 17/50 -- Step: 201 -- Batch: 321/469 -- Disc Loss: 0.4471 -- Gen Loss: 3.5798\n",
      "Epoch: 17/50 -- Step: 202 -- Batch: 361/469 -- Disc Loss: 0.2173 -- Gen Loss: 3.3662\n",
      "Epoch: 17/50 -- Step: 203 -- Batch: 401/469 -- Disc Loss: 0.3675 -- Gen Loss: 2.3783\n",
      "Epoch: 17/50 -- Step: 204 -- Batch: 441/469 -- Disc Loss: 0.5181 -- Gen Loss: 2.0661\n",
      "Epoch: 18/50 -- Step: 205 -- Batch: 1/469 -- Disc Loss: 0.8287 -- Gen Loss: 4.2641\n",
      "Epoch: 18/50 -- Step: 206 -- Batch: 41/469 -- Disc Loss: 0.2406 -- Gen Loss: 2.5583\n",
      "Epoch: 18/50 -- Step: 207 -- Batch: 81/469 -- Disc Loss: 0.3031 -- Gen Loss: 2.6041\n",
      "Epoch: 18/50 -- Step: 208 -- Batch: 121/469 -- Disc Loss: 1.7297 -- Gen Loss: 2.7494\n",
      "Epoch: 18/50 -- Step: 209 -- Batch: 161/469 -- Disc Loss: 0.3311 -- Gen Loss: 2.3419\n",
      "Epoch: 18/50 -- Step: 210 -- Batch: 201/469 -- Disc Loss: 0.4036 -- Gen Loss: 2.2747\n",
      "Epoch: 18/50 -- Step: 211 -- Batch: 241/469 -- Disc Loss: 0.4286 -- Gen Loss: 4.1766\n",
      "Epoch: 18/50 -- Step: 212 -- Batch: 281/469 -- Disc Loss: 0.6329 -- Gen Loss: 2.9500\n",
      "Epoch: 18/50 -- Step: 213 -- Batch: 321/469 -- Disc Loss: 0.3530 -- Gen Loss: 3.3935\n",
      "Epoch: 18/50 -- Step: 214 -- Batch: 361/469 -- Disc Loss: 0.6306 -- Gen Loss: 3.6557\n",
      "Epoch: 18/50 -- Step: 215 -- Batch: 401/469 -- Disc Loss: 0.4233 -- Gen Loss: 3.1239\n",
      "Epoch: 18/50 -- Step: 216 -- Batch: 441/469 -- Disc Loss: 1.1842 -- Gen Loss: 4.9360\n",
      "Epoch: 19/50 -- Step: 217 -- Batch: 1/469 -- Disc Loss: 0.6422 -- Gen Loss: 4.4192\n",
      "Epoch: 19/50 -- Step: 218 -- Batch: 41/469 -- Disc Loss: 0.5657 -- Gen Loss: 1.9643\n",
      "Epoch: 19/50 -- Step: 219 -- Batch: 81/469 -- Disc Loss: 0.4188 -- Gen Loss: 3.0126\n",
      "Epoch: 19/50 -- Step: 220 -- Batch: 121/469 -- Disc Loss: 0.5618 -- Gen Loss: 1.4819\n",
      "Epoch: 19/50 -- Step: 221 -- Batch: 161/469 -- Disc Loss: 0.2511 -- Gen Loss: 2.6777\n",
      "Epoch: 19/50 -- Step: 222 -- Batch: 201/469 -- Disc Loss: 0.3708 -- Gen Loss: 3.0574\n",
      "Epoch: 19/50 -- Step: 223 -- Batch: 241/469 -- Disc Loss: 0.4214 -- Gen Loss: 4.0823\n",
      "Epoch: 19/50 -- Step: 224 -- Batch: 281/469 -- Disc Loss: 0.4924 -- Gen Loss: 3.8312\n",
      "Epoch: 19/50 -- Step: 225 -- Batch: 321/469 -- Disc Loss: 0.3397 -- Gen Loss: 3.3695\n",
      "Epoch: 19/50 -- Step: 226 -- Batch: 361/469 -- Disc Loss: 0.5757 -- Gen Loss: 2.7017\n",
      "Epoch: 19/50 -- Step: 227 -- Batch: 401/469 -- Disc Loss: 0.5014 -- Gen Loss: 1.3560\n",
      "Epoch: 19/50 -- Step: 228 -- Batch: 441/469 -- Disc Loss: 0.5986 -- Gen Loss: 4.8617\n",
      "Epoch: 20/50 -- Step: 229 -- Batch: 1/469 -- Disc Loss: 0.4387 -- Gen Loss: 2.1409\n",
      "Epoch: 20/50 -- Step: 230 -- Batch: 41/469 -- Disc Loss: 0.5490 -- Gen Loss: 2.6727\n",
      "Epoch: 20/50 -- Step: 231 -- Batch: 81/469 -- Disc Loss: 0.2938 -- Gen Loss: 2.4091\n",
      "Epoch: 20/50 -- Step: 232 -- Batch: 121/469 -- Disc Loss: 0.5876 -- Gen Loss: 3.0633\n",
      "Epoch: 20/50 -- Step: 233 -- Batch: 161/469 -- Disc Loss: 0.3748 -- Gen Loss: 4.4107\n",
      "Epoch: 20/50 -- Step: 234 -- Batch: 201/469 -- Disc Loss: 0.4572 -- Gen Loss: 3.1788\n",
      "Epoch: 20/50 -- Step: 235 -- Batch: 241/469 -- Disc Loss: 0.5478 -- Gen Loss: 1.6493\n",
      "Epoch: 20/50 -- Step: 236 -- Batch: 281/469 -- Disc Loss: 0.1543 -- Gen Loss: 3.5651\n",
      "Epoch: 20/50 -- Step: 237 -- Batch: 321/469 -- Disc Loss: 0.4418 -- Gen Loss: 2.3509\n",
      "Epoch: 20/50 -- Step: 238 -- Batch: 361/469 -- Disc Loss: 0.3491 -- Gen Loss: 3.4185\n",
      "Epoch: 20/50 -- Step: 239 -- Batch: 401/469 -- Disc Loss: 0.4485 -- Gen Loss: 2.4928\n",
      "Epoch: 20/50 -- Step: 240 -- Batch: 441/469 -- Disc Loss: 0.4176 -- Gen Loss: 2.7791\n",
      "Epoch: 21/50 -- Step: 241 -- Batch: 1/469 -- Disc Loss: 0.2778 -- Gen Loss: 3.3019\n",
      "Epoch: 21/50 -- Step: 242 -- Batch: 41/469 -- Disc Loss: 0.4475 -- Gen Loss: 3.5400\n",
      "Epoch: 21/50 -- Step: 243 -- Batch: 81/469 -- Disc Loss: 0.5749 -- Gen Loss: 1.7149\n",
      "Epoch: 21/50 -- Step: 244 -- Batch: 121/469 -- Disc Loss: 0.3115 -- Gen Loss: 2.6324\n",
      "Epoch: 21/50 -- Step: 245 -- Batch: 161/469 -- Disc Loss: 0.3998 -- Gen Loss: 3.0884\n",
      "Epoch: 21/50 -- Step: 246 -- Batch: 201/469 -- Disc Loss: 0.4172 -- Gen Loss: 3.5987\n",
      "Epoch: 21/50 -- Step: 247 -- Batch: 241/469 -- Disc Loss: 0.5883 -- Gen Loss: 2.0169\n",
      "Epoch: 21/50 -- Step: 248 -- Batch: 281/469 -- Disc Loss: 0.1871 -- Gen Loss: 2.4873\n",
      "Epoch: 21/50 -- Step: 249 -- Batch: 321/469 -- Disc Loss: 0.4221 -- Gen Loss: 2.5944\n",
      "Epoch: 21/50 -- Step: 250 -- Batch: 361/469 -- Disc Loss: 0.3501 -- Gen Loss: 3.6127\n",
      "Epoch: 21/50 -- Step: 251 -- Batch: 401/469 -- Disc Loss: 0.5486 -- Gen Loss: 4.3264\n",
      "Epoch: 21/50 -- Step: 252 -- Batch: 441/469 -- Disc Loss: 0.2860 -- Gen Loss: 3.0290\n",
      "Epoch: 22/50 -- Step: 253 -- Batch: 1/469 -- Disc Loss: 0.5149 -- Gen Loss: 2.8741\n",
      "Epoch: 22/50 -- Step: 254 -- Batch: 41/469 -- Disc Loss: 0.5180 -- Gen Loss: 2.1072\n",
      "Epoch: 22/50 -- Step: 255 -- Batch: 81/469 -- Disc Loss: 0.1897 -- Gen Loss: 3.4155\n",
      "Epoch: 22/50 -- Step: 256 -- Batch: 121/469 -- Disc Loss: 0.3049 -- Gen Loss: 2.6770\n",
      "Epoch: 22/50 -- Step: 257 -- Batch: 161/469 -- Disc Loss: 0.2235 -- Gen Loss: 4.3645\n",
      "Epoch: 22/50 -- Step: 258 -- Batch: 201/469 -- Disc Loss: 0.1626 -- Gen Loss: 2.6383\n",
      "Epoch: 22/50 -- Step: 259 -- Batch: 241/469 -- Disc Loss: 0.5342 -- Gen Loss: 2.6038\n",
      "Epoch: 22/50 -- Step: 260 -- Batch: 281/469 -- Disc Loss: 0.3641 -- Gen Loss: 3.2943\n",
      "Epoch: 22/50 -- Step: 261 -- Batch: 321/469 -- Disc Loss: 0.3752 -- Gen Loss: 3.3007\n",
      "Epoch: 22/50 -- Step: 262 -- Batch: 361/469 -- Disc Loss: 0.2403 -- Gen Loss: 2.5740\n",
      "Epoch: 22/50 -- Step: 263 -- Batch: 401/469 -- Disc Loss: 0.3149 -- Gen Loss: 1.7463\n",
      "Epoch: 22/50 -- Step: 264 -- Batch: 441/469 -- Disc Loss: 0.2528 -- Gen Loss: 3.2868\n",
      "Epoch: 23/50 -- Step: 265 -- Batch: 1/469 -- Disc Loss: 0.2704 -- Gen Loss: 2.5402\n",
      "Epoch: 23/50 -- Step: 266 -- Batch: 41/469 -- Disc Loss: 1.5709 -- Gen Loss: 3.9059\n",
      "Epoch: 23/50 -- Step: 267 -- Batch: 81/469 -- Disc Loss: 0.4742 -- Gen Loss: 2.1025\n",
      "Epoch: 23/50 -- Step: 268 -- Batch: 121/469 -- Disc Loss: 0.4499 -- Gen Loss: 2.1999\n",
      "Epoch: 23/50 -- Step: 269 -- Batch: 161/469 -- Disc Loss: 0.2260 -- Gen Loss: 2.5124\n",
      "Epoch: 23/50 -- Step: 270 -- Batch: 201/469 -- Disc Loss: 0.3355 -- Gen Loss: 2.9826\n",
      "Epoch: 23/50 -- Step: 271 -- Batch: 241/469 -- Disc Loss: 0.2378 -- Gen Loss: 2.9292\n",
      "Epoch: 23/50 -- Step: 272 -- Batch: 281/469 -- Disc Loss: 0.6458 -- Gen Loss: 4.5389\n",
      "Epoch: 23/50 -- Step: 273 -- Batch: 321/469 -- Disc Loss: 0.8967 -- Gen Loss: 4.6255\n",
      "Epoch: 23/50 -- Step: 274 -- Batch: 361/469 -- Disc Loss: 0.2308 -- Gen Loss: 3.7115\n",
      "Epoch: 23/50 -- Step: 275 -- Batch: 401/469 -- Disc Loss: 0.6867 -- Gen Loss: 2.0237\n",
      "Epoch: 23/50 -- Step: 276 -- Batch: 441/469 -- Disc Loss: 0.1103 -- Gen Loss: 3.7735\n",
      "Epoch: 24/50 -- Step: 277 -- Batch: 1/469 -- Disc Loss: 0.3694 -- Gen Loss: 4.1847\n",
      "Epoch: 24/50 -- Step: 278 -- Batch: 41/469 -- Disc Loss: 0.5506 -- Gen Loss: 5.0519\n",
      "Epoch: 24/50 -- Step: 279 -- Batch: 81/469 -- Disc Loss: 0.1907 -- Gen Loss: 2.9835\n",
      "Epoch: 24/50 -- Step: 280 -- Batch: 121/469 -- Disc Loss: 0.3394 -- Gen Loss: 3.4383\n",
      "Epoch: 24/50 -- Step: 281 -- Batch: 161/469 -- Disc Loss: 0.3453 -- Gen Loss: 2.4172\n",
      "Epoch: 24/50 -- Step: 282 -- Batch: 201/469 -- Disc Loss: 0.2366 -- Gen Loss: 4.0091\n",
      "Epoch: 24/50 -- Step: 283 -- Batch: 241/469 -- Disc Loss: 0.1825 -- Gen Loss: 2.7189\n",
      "Epoch: 24/50 -- Step: 284 -- Batch: 281/469 -- Disc Loss: 0.2609 -- Gen Loss: 2.3477\n",
      "Epoch: 24/50 -- Step: 285 -- Batch: 321/469 -- Disc Loss: 0.2011 -- Gen Loss: 3.4993\n",
      "Epoch: 24/50 -- Step: 286 -- Batch: 361/469 -- Disc Loss: 0.4181 -- Gen Loss: 2.7305\n",
      "Epoch: 24/50 -- Step: 287 -- Batch: 401/469 -- Disc Loss: 0.3458 -- Gen Loss: 4.4151\n",
      "Epoch: 24/50 -- Step: 288 -- Batch: 441/469 -- Disc Loss: 0.2969 -- Gen Loss: 2.7907\n",
      "Epoch: 25/50 -- Step: 289 -- Batch: 1/469 -- Disc Loss: 0.5338 -- Gen Loss: 4.5520\n",
      "Epoch: 25/50 -- Step: 290 -- Batch: 41/469 -- Disc Loss: 0.2151 -- Gen Loss: 4.0711\n",
      "Epoch: 25/50 -- Step: 291 -- Batch: 81/469 -- Disc Loss: 0.3573 -- Gen Loss: 4.3420\n",
      "Epoch: 25/50 -- Step: 292 -- Batch: 121/469 -- Disc Loss: 0.2830 -- Gen Loss: 3.2306\n",
      "Epoch: 25/50 -- Step: 293 -- Batch: 161/469 -- Disc Loss: 0.2435 -- Gen Loss: 3.4239\n",
      "Epoch: 25/50 -- Step: 294 -- Batch: 201/469 -- Disc Loss: 0.2989 -- Gen Loss: 2.6832\n",
      "Epoch: 25/50 -- Step: 295 -- Batch: 241/469 -- Disc Loss: 0.3931 -- Gen Loss: 2.5319\n",
      "Epoch: 25/50 -- Step: 296 -- Batch: 281/469 -- Disc Loss: 0.2337 -- Gen Loss: 3.2167\n",
      "Epoch: 25/50 -- Step: 297 -- Batch: 321/469 -- Disc Loss: 0.4528 -- Gen Loss: 1.3980\n",
      "Epoch: 25/50 -- Step: 298 -- Batch: 361/469 -- Disc Loss: 0.2494 -- Gen Loss: 4.2564\n",
      "Epoch: 25/50 -- Step: 299 -- Batch: 401/469 -- Disc Loss: 1.0221 -- Gen Loss: 0.4412\n",
      "Epoch: 25/50 -- Step: 300 -- Batch: 441/469 -- Disc Loss: 0.4025 -- Gen Loss: 3.7161\n",
      "Epoch: 26/50 -- Step: 301 -- Batch: 1/469 -- Disc Loss: 0.9665 -- Gen Loss: 7.0963\n",
      "Epoch: 26/50 -- Step: 302 -- Batch: 41/469 -- Disc Loss: 0.3046 -- Gen Loss: 2.9586\n",
      "Epoch: 26/50 -- Step: 303 -- Batch: 81/469 -- Disc Loss: 0.2397 -- Gen Loss: 2.9362\n",
      "Epoch: 26/50 -- Step: 304 -- Batch: 121/469 -- Disc Loss: 0.4652 -- Gen Loss: 4.6439\n",
      "Epoch: 26/50 -- Step: 305 -- Batch: 161/469 -- Disc Loss: 0.2016 -- Gen Loss: 2.8018\n",
      "Epoch: 26/50 -- Step: 306 -- Batch: 201/469 -- Disc Loss: 0.1299 -- Gen Loss: 3.4271\n",
      "Epoch: 26/50 -- Step: 307 -- Batch: 241/469 -- Disc Loss: 0.4394 -- Gen Loss: 2.2756\n",
      "Epoch: 26/50 -- Step: 308 -- Batch: 281/469 -- Disc Loss: 0.3339 -- Gen Loss: 3.2015\n",
      "Epoch: 26/50 -- Step: 309 -- Batch: 321/469 -- Disc Loss: 0.2562 -- Gen Loss: 3.7910\n",
      "Epoch: 26/50 -- Step: 310 -- Batch: 361/469 -- Disc Loss: 0.2744 -- Gen Loss: 2.9298\n",
      "Epoch: 26/50 -- Step: 311 -- Batch: 401/469 -- Disc Loss: 0.2257 -- Gen Loss: 2.9052\n",
      "Epoch: 26/50 -- Step: 312 -- Batch: 441/469 -- Disc Loss: 0.1733 -- Gen Loss: 3.2329\n",
      "Epoch: 27/50 -- Step: 313 -- Batch: 1/469 -- Disc Loss: 1.6349 -- Gen Loss: 2.1506\n",
      "Epoch: 27/50 -- Step: 314 -- Batch: 41/469 -- Disc Loss: 0.3124 -- Gen Loss: 4.7383\n",
      "Epoch: 27/50 -- Step: 315 -- Batch: 81/469 -- Disc Loss: 0.3576 -- Gen Loss: 2.6558\n",
      "Epoch: 27/50 -- Step: 316 -- Batch: 121/469 -- Disc Loss: 0.8539 -- Gen Loss: 6.8368\n",
      "Epoch: 27/50 -- Step: 317 -- Batch: 161/469 -- Disc Loss: 0.3341 -- Gen Loss: 2.5525\n",
      "Epoch: 27/50 -- Step: 318 -- Batch: 201/469 -- Disc Loss: 0.1516 -- Gen Loss: 3.5301\n",
      "Epoch: 27/50 -- Step: 319 -- Batch: 241/469 -- Disc Loss: 0.3906 -- Gen Loss: 6.4705\n",
      "Epoch: 27/50 -- Step: 320 -- Batch: 281/469 -- Disc Loss: 0.2194 -- Gen Loss: 3.9797\n",
      "Epoch: 27/50 -- Step: 321 -- Batch: 321/469 -- Disc Loss: 0.2971 -- Gen Loss: 2.3929\n",
      "Epoch: 27/50 -- Step: 322 -- Batch: 361/469 -- Disc Loss: 0.2234 -- Gen Loss: 3.3983\n",
      "Epoch: 27/50 -- Step: 323 -- Batch: 401/469 -- Disc Loss: 1.7073 -- Gen Loss: 2.4965\n",
      "Epoch: 27/50 -- Step: 324 -- Batch: 441/469 -- Disc Loss: 0.1138 -- Gen Loss: 3.8845\n",
      "Epoch: 28/50 -- Step: 325 -- Batch: 1/469 -- Disc Loss: 0.1437 -- Gen Loss: 3.3778\n",
      "Epoch: 28/50 -- Step: 326 -- Batch: 41/469 -- Disc Loss: 0.3504 -- Gen Loss: 2.9333\n",
      "Epoch: 28/50 -- Step: 327 -- Batch: 81/469 -- Disc Loss: 0.1995 -- Gen Loss: 3.4413\n",
      "Epoch: 28/50 -- Step: 328 -- Batch: 121/469 -- Disc Loss: 0.1430 -- Gen Loss: 3.4273\n",
      "Epoch: 28/50 -- Step: 329 -- Batch: 161/469 -- Disc Loss: 0.2081 -- Gen Loss: 3.0370\n",
      "Epoch: 28/50 -- Step: 330 -- Batch: 201/469 -- Disc Loss: 0.2979 -- Gen Loss: 2.5883\n",
      "Epoch: 28/50 -- Step: 331 -- Batch: 241/469 -- Disc Loss: 0.4587 -- Gen Loss: 3.6600\n",
      "Epoch: 28/50 -- Step: 332 -- Batch: 281/469 -- Disc Loss: 0.1751 -- Gen Loss: 3.6815\n",
      "Epoch: 28/50 -- Step: 333 -- Batch: 321/469 -- Disc Loss: 0.3087 -- Gen Loss: 3.6466\n",
      "Epoch: 28/50 -- Step: 334 -- Batch: 361/469 -- Disc Loss: 0.3821 -- Gen Loss: 2.4449\n",
      "Epoch: 28/50 -- Step: 335 -- Batch: 401/469 -- Disc Loss: 0.3650 -- Gen Loss: 2.6191\n",
      "Epoch: 28/50 -- Step: 336 -- Batch: 441/469 -- Disc Loss: 0.1214 -- Gen Loss: 3.5857\n",
      "Epoch: 29/50 -- Step: 337 -- Batch: 1/469 -- Disc Loss: 0.3220 -- Gen Loss: 5.0500\n",
      "Epoch: 29/50 -- Step: 338 -- Batch: 41/469 -- Disc Loss: 0.2069 -- Gen Loss: 3.8273\n",
      "Epoch: 29/50 -- Step: 339 -- Batch: 81/469 -- Disc Loss: 0.6678 -- Gen Loss: 4.4897\n",
      "Epoch: 29/50 -- Step: 340 -- Batch: 121/469 -- Disc Loss: 2.1260 -- Gen Loss: 4.5821\n",
      "Epoch: 29/50 -- Step: 341 -- Batch: 161/469 -- Disc Loss: 1.4534 -- Gen Loss: 4.5298\n",
      "Epoch: 29/50 -- Step: 342 -- Batch: 201/469 -- Disc Loss: 0.4466 -- Gen Loss: 3.2947\n",
      "Epoch: 29/50 -- Step: 343 -- Batch: 241/469 -- Disc Loss: 0.2275 -- Gen Loss: 3.4235\n",
      "Epoch: 29/50 -- Step: 344 -- Batch: 281/469 -- Disc Loss: 0.1058 -- Gen Loss: 3.2485\n",
      "Epoch: 29/50 -- Step: 345 -- Batch: 321/469 -- Disc Loss: 0.0805 -- Gen Loss: 5.1210\n",
      "Epoch: 29/50 -- Step: 346 -- Batch: 361/469 -- Disc Loss: 0.1562 -- Gen Loss: 2.8604\n",
      "Epoch: 29/50 -- Step: 347 -- Batch: 401/469 -- Disc Loss: 0.2849 -- Gen Loss: 2.9427\n",
      "Epoch: 29/50 -- Step: 348 -- Batch: 441/469 -- Disc Loss: 0.4252 -- Gen Loss: 2.6138\n",
      "Epoch: 30/50 -- Step: 349 -- Batch: 1/469 -- Disc Loss: 0.1973 -- Gen Loss: 3.8502\n",
      "Epoch: 30/50 -- Step: 350 -- Batch: 41/469 -- Disc Loss: 0.1442 -- Gen Loss: 3.6764\n",
      "Epoch: 30/50 -- Step: 351 -- Batch: 81/469 -- Disc Loss: 0.1975 -- Gen Loss: 2.4498\n",
      "Epoch: 30/50 -- Step: 352 -- Batch: 121/469 -- Disc Loss: 0.1418 -- Gen Loss: 4.1033\n",
      "Epoch: 30/50 -- Step: 353 -- Batch: 161/469 -- Disc Loss: 0.1579 -- Gen Loss: 4.4549\n",
      "Epoch: 30/50 -- Step: 354 -- Batch: 201/469 -- Disc Loss: 0.2799 -- Gen Loss: 2.8671\n",
      "Epoch: 30/50 -- Step: 355 -- Batch: 241/469 -- Disc Loss: 0.0926 -- Gen Loss: 4.1639\n",
      "Epoch: 30/50 -- Step: 356 -- Batch: 281/469 -- Disc Loss: 0.1516 -- Gen Loss: 3.3342\n",
      "Epoch: 30/50 -- Step: 357 -- Batch: 321/469 -- Disc Loss: 0.3694 -- Gen Loss: 3.9007\n",
      "Epoch: 31/50 -- Step: 358 -- Batch: 1/469 -- Disc Loss: 0.1141 -- Gen Loss: 3.2143\n",
      "Epoch: 31/50 -- Step: 359 -- Batch: 41/469 -- Disc Loss: 0.4704 -- Gen Loss: 2.9414\n",
      "Epoch: 31/50 -- Step: 360 -- Batch: 81/469 -- Disc Loss: 0.2089 -- Gen Loss: 3.2006\n",
      "Epoch: 31/50 -- Step: 361 -- Batch: 121/469 -- Disc Loss: 0.1765 -- Gen Loss: 2.6045\n",
      "Epoch: 31/50 -- Step: 362 -- Batch: 161/469 -- Disc Loss: 0.4457 -- Gen Loss: 6.7679\n",
      "Epoch: 31/50 -- Step: 363 -- Batch: 201/469 -- Disc Loss: 0.1603 -- Gen Loss: 3.8316\n",
      "Epoch: 31/50 -- Step: 364 -- Batch: 241/469 -- Disc Loss: 0.2501 -- Gen Loss: 4.6522\n",
      "Epoch: 31/50 -- Step: 365 -- Batch: 281/469 -- Disc Loss: 0.1534 -- Gen Loss: 3.9269\n",
      "Epoch: 31/50 -- Step: 366 -- Batch: 321/469 -- Disc Loss: 0.3150 -- Gen Loss: 4.9126\n",
      "Epoch: 31/50 -- Step: 367 -- Batch: 361/469 -- Disc Loss: 0.1848 -- Gen Loss: 3.9943\n",
      "Epoch: 31/50 -- Step: 368 -- Batch: 401/469 -- Disc Loss: 0.3152 -- Gen Loss: 4.6044\n",
      "Epoch: 31/50 -- Step: 369 -- Batch: 441/469 -- Disc Loss: 0.1258 -- Gen Loss: 4.5258\n",
      "Epoch: 32/50 -- Step: 370 -- Batch: 1/469 -- Disc Loss: 0.1300 -- Gen Loss: 4.0437\n",
      "Epoch: 32/50 -- Step: 371 -- Batch: 41/469 -- Disc Loss: 0.2465 -- Gen Loss: 3.4781\n",
      "Epoch: 32/50 -- Step: 372 -- Batch: 81/469 -- Disc Loss: 0.2082 -- Gen Loss: 2.2006\n",
      "Epoch: 32/50 -- Step: 373 -- Batch: 121/469 -- Disc Loss: 0.1042 -- Gen Loss: 4.0719\n",
      "Epoch: 32/50 -- Step: 374 -- Batch: 161/469 -- Disc Loss: 0.2045 -- Gen Loss: 4.2025\n",
      "Epoch: 32/50 -- Step: 375 -- Batch: 201/469 -- Disc Loss: 0.1102 -- Gen Loss: 4.3528\n",
      "Epoch: 32/50 -- Step: 376 -- Batch: 241/469 -- Disc Loss: 0.0997 -- Gen Loss: 4.7681\n",
      "Epoch: 32/50 -- Step: 377 -- Batch: 281/469 -- Disc Loss: 0.1871 -- Gen Loss: 3.4330\n",
      "Epoch: 32/50 -- Step: 378 -- Batch: 321/469 -- Disc Loss: 0.1891 -- Gen Loss: 4.9049\n",
      "Epoch: 32/50 -- Step: 379 -- Batch: 361/469 -- Disc Loss: 0.2027 -- Gen Loss: 4.1645\n",
      "Epoch: 32/50 -- Step: 380 -- Batch: 401/469 -- Disc Loss: 0.0808 -- Gen Loss: 3.7934\n",
      "Epoch: 32/50 -- Step: 381 -- Batch: 441/469 -- Disc Loss: 0.1177 -- Gen Loss: 3.2490\n",
      "Epoch: 33/50 -- Step: 382 -- Batch: 1/469 -- Disc Loss: 0.1367 -- Gen Loss: 4.3848\n"
     ]
    }
   ],
   "source": [
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real, labels) in enumerate(images_loader):\n",
    "        real = real.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        for _ in range(DISC_ITER):\n",
    "            noise = torch.randn((real.shape[0], Z_DIM, 1, 1)).to(DEVICE)\n",
    "            fake = gen(noise, labels)\n",
    "            disc_real = disc(real, labels).view(-1)\n",
    "            loss_disc_real = criterion(\n",
    "                input=disc_real,\n",
    "                target=torch.ones_like(disc_real),\n",
    "            )\n",
    "            disc_fake = disc(fake.detach(), labels).view(-1)\n",
    "            loss_disc_fake = criterion(\n",
    "                input=disc_fake,\n",
    "                target=torch.zeros_like(disc_fake),\n",
    "            )\n",
    "            loss_disc = (loss_disc_real + loss_disc_fake)\n",
    "            disc.zero_grad()\n",
    "            loss_disc.backward()\n",
    "            optim_disc.step()\n",
    "            \n",
    "        disc_fake = disc(fake, labels).view(-1)\n",
    "        loss_gen = criterion(\n",
    "            input=disc_fake,\n",
    "            target=torch.ones_like(disc_fake),\n",
    "        )\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        optim_gen.step()\n",
    "\n",
    "        if batch_idx % 40 == 0:\n",
    "            print(\n",
    "                f'Epoch: {epoch+1}/{NUM_EPOCHS} -- Step: {steps} -- Batch: {batch_idx+1}/{len(images_loader)} -- Disc Loss: {loss_disc:.4f} -- Gen Loss: {loss_gen:.4f}'\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                fixed_noise = torch.randn((GRID_SHOW, Z_DIM, 1, 1)).to(DEVICE)\n",
    "\n",
    "                fake = gen(fixed_noise, fixed_labels)\n",
    "                fake_images = torchvision.utils.make_grid(fake, nrow=5, normalize=True)\n",
    "                save_image(fake_images, f'dcgan_results/{steps}.png')\n",
    "                real_images = torchvision.utils.make_grid(real[:GRID_SHOW], nrow=5, normalize=True)\n",
    "                writer_fake.add_scalar('Gen Loss', loss_gen, global_step=steps)\n",
    "                writer_fake.add_image(\n",
    "                    'Fake', fake_images, global_step=steps\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    'Real', real_images, global_step=steps\n",
    "                )\n",
    "                steps += 1\n",
    "                if steps == 358:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
